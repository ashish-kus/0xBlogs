[{"content":"Setting Up a Windows 11 VM with virt-manager (Without Losing Your Mind) Introduction So you want to run Windows 11 in a virtual machine on Linux? Excellent choice! Whether you need it for testing, running that one annoying Windows-only app, or just want to see what Microsoft is up to these days, this guide will walk you through creating a properly optimized Windows 11 VM using virt-manager.\nThe beauty of what we are doing today is that we\u0026rsquo;re going to use VirtIO drivers (which are way faster than the default emulated hardware) and add some Hyper-V enlightenments to make Windows think it\u0026rsquo;s running on native Microsoft hypervisor technology. This makes everything smoother and faster.\nRequired Downloads and Setup 1. Virtual Machine Manager (virt-manager) This is what we‚Äôll use to create and manage our virtual machines. It sits on top of QEMU/KVM, which does the actual virtualization magic, and provides a nice UI so you don‚Äôt have to memorize terminal incantations. To starts the virtualization daemon that virt-manager uses behind the scenes to control VMs. Add user to that group. One of the most important part is gives your VMs internet access ‚Äî basically a mini router inside your system.\nsudo pacman -S virt-manager qemu-desktop dnsmasq vde2 bridge-utils openbsd-netcat sudo systemctl enable --now libvirtd.service sudo usermod -aG libvirt $USER sudo virsh net-start default sudo virsh net-autostart default 2. Windows 11 ISO Head over to the official Microsoft download page and grab the Windows 11 ISO. Yes, it‚Äôs free to download ‚Äî the activation popup comes later to remind you of capitalism. This ISO is what you‚Äôll ‚Äúinsert‚Äù into your virtual DVD drive to install Windows inside the VM.\n3. VirtIO Drivers ISO Download this from the Fedora Project VirtIO Drivers page These are not optional ‚Äî they‚Äôre the drivers that make Windows actually usable inside KVM. Without them, you‚Äôll experience prehistoric speeds and missing devices. Yes it will take 1000 years.\n4. Spice Guest Tools (for post-install) Download from spice-space.org. This gives you proper display drivers, clipboard sharing, and other quality-of-life features\n5. Setting up TPM 2.0 (for Windows 11) Windows 11 demands TPM 2.0 and Secure Boot ‚Äî which are security features for encryption, BitLocker, and secure authentication. In the VM world, TPM is emulated ‚Äî meaning we fake it (and Windows is fine with that).\nsudo pacman -S swtpm edk2-ovmf swtpm ‚Üí Software TPM emulator; this tricks Windows into thinking your VM has a real TPM chip. edk2-ovmf ‚Üí Provides UEFI firmware (needed for Secure Boot \u0026amp; modern installations). Once installed, you can enable TPM 2.0 and UEFI from the ‚ÄúOverview‚Äù tab in virt-manager when editing your VM settings. System Preparation Before we dive in, make sure virtualization is actually enabled: BIOS Settings: Boot into your BIOS/UEFI and enable virtualization Look for Intel VT-x or AMD-V (different names, same idea) Sometimes it\u0026rsquo;s hiding under \u0026ldquo;SVM Mode\u0026rdquo; or \u0026ldquo;Virtualization Technology\u0026rdquo; Why? Because your CPU has special hardware acceleration for VMs, and you definitely want to use it\nCreating the VM: The Fun Setup Part Initial Setup (Pre installations) Open virt-manager and create a new VM, but here\u0026rsquo;s the trick: don\u0026rsquo;t click \u0026ldquo;Finish\u0026rdquo; yet!. Check the box that says \u0026ldquo;Customize configuration before install\u0026rdquo;. This is crucial because we need to tweak a bunch of settings before Windows even boots\nEnable XML editing (you\u0026rsquo;ll need this later):\nGo to Edit ‚Üí Preferences ‚Üí Enable XML editing. Don\u0026rsquo;t worry, you won\u0026rsquo;t need to write XML from scratch. We\u0026rsquo;re just copying and pasting some magic incantations.\nOverview Ensure Chipset: Q35 and Firmwre: UEFI: Q35 is the modern chipset emulation that supports PCIe properly, and UEFI is what Windows 11 expects (it won\u0026rsquo;t even install with legacy BIOS). If you have an AMD CPU, remove the \u0026lt;evmcs state=\u0026quot;on\u0026quot;/\u0026gt; line. That\u0026rsquo;s an Intel-only feature. Find the section and add this \u0026lt;timer name=\u0026quot;hypervclock\u0026quot; present=\u0026quot;yes\u0026quot;/\u0026gt;, This gives Windows access to a Hyper-V synthetic timer. CPU Enable host-passthrough : Instead of emulating a generic CPU, this passes through your actual CPU model to the VM. Windows gets to see and use all your CPU\u0026rsquo;s fancy features, which means better performance and compatibility. Storage Setup Disk bus: VirtIO: Way faster than emulated SATA (we\u0026rsquo;re talking 5-10x faster I/O). Cache mode: none: Ensures data integrity by bypassing the host\u0026rsquo;s page cache. Discard mode : Allows TRIM commands to work, so deleted files actually free up space on your host drive. Mount the VirtIO Driver ISO Add Hardware Browse to your virtio-win.iso and Device type: CDROM device : Windows has no idea what VirtIO devices are, so we need to load drivers during installation. This ISO contains all those drivers. Network Configuration Device model: virtio : Same reason as the disk‚ÄîVirtIO network is dramatically faster than emulated Intel or Realtek cards. Other Configuration Add Guest Agent Channel: Add Hardware -\u0026gt; Channel -\u0026gt; Name: org.qemu.guest_agent.0 This lets your host and guest communicate for things like graceful shutdowns and status reporting. Remove the Tablet device : It\u0026rsquo;s redundant and we don\u0026rsquo;t need it. Enable TPM : Type: Emulated, Version 2.0. Windows 11 literally refuses to install without a TPM. This emulates one so Microsoft\u0026rsquo;s installer stops complaining. Windows Installation: The Obstacle Course Disk Partitioning When you get to the disk selection screen, Windows will act confused and claim it can\u0026rsquo;t find any disks. Don\u0026rsquo;t panic!\nClick \u0026ldquo;Load driver\u0026rdquo; Browse to E:\\viostor\\w11\\amd64 (or whatever drive letter the VirtIO ISO got) Click OK Your disk should magically appear now! Optional but recommended Also load the network driver the same way from E:\\NetKVM\\w11\\amd64. You can do this now or after installation. Bypassing Microsoft Account Requirement Microsoft really wants you to use a Microsoft account. We\u0026rsquo;re going to politely decline:\nWhen installation becomes graphical and asks for your region, press Shift+F10 Type: OOBE\\BYPASSNRO Wait for the automatic restart This command tells Windows to run the Out-of-Box Experience with Network Requirement Overrides disabled. Fancy name, simple result: you get a \u0026ldquo;I don\u0026rsquo;t have internet\u0026rdquo; option. Post-Installation: Making It Actually Good Once Windows is up and running, it‚Äôs time to make it fast and fully functional.\nOpen File Explorer ‚Üí navigate to the VirtIO CD (usually E:) Double-click virtio-win-gt-x64.msi Follow the prompts and install everything\nüí° What this does: This package installs all the essential VirtIO drivers ‚Äî storage, network, and balloon (for dynamic memory), plus the QEMU guest agent. Together, they make your VM feel like real hardware instead of an emulation museum exhibit.\nOnce done, eject the VirtIO CD from your VM settings ‚Äî you won‚Äôt need it again.\nSure ‚Äî here‚Äôs a cleaner, more engaging rewrite with a short explanation of why Spice matters:\nInstall Spice Guest Tools The secret sauce that makes your Windows VM actually pleasant to use.\nRun the Spice Guest Tools installer you downloaded earlier from spice-space.org Follow the on-screen setup ‚Äî quick and painless üí° Why Spice is important: Without Spice, your VM feels like remote-controlling a computer over a bad Wi-Fi connection. With it, everything becomes seamless ‚Äî smooth graphics, fast mouse movement, and proper host‚Äìguest integration.\nAfter installation, you‚Äôll get:\nAuto-adjusting display resolution (resize the VM window, and Windows adapts instantly) Bidirectional clipboard (copy/paste between host and VM) Better mouse integration (no more cursor lag or capture issues) Drag-and-drop file transfer between host and guest Basically, Spice turns your VM from ‚Äúusable‚Äù to ‚Äúactually enjoyable.‚Äù\nSure! Here‚Äôs a smoother, more natural paragraph-style version:\nOptional: Enable 3D Acceleration If you want your Windows VM to feel a bit more alive ‚Äî with smoother animations and faster window rendering ‚Äî enabling 3D acceleration is worth it. First, shut down your VM completely. Then, open its settings in virt-manager and under the Display section, select Spice, set the Listen Type to None, and enable OpenGL. Next, go to the Video section, switch the model to Virtio, and check the box for 3D acceleration.\nüí° This allows Windows to tap into your host‚Äôs GPU for OpenGL rendering. You won‚Äôt get gaming-level performance, but the desktop experience becomes noticeably more fluid and responsive ‚Äî perfect for daily use.\nOptimizing Windows 11: Because It Needs Help Fresh Windows 11 installs come loaded with background services, telemetry, and unwanted features. Here\u0026rsquo;s how to make it lean and VM-friendly.\nDisable Copilot Via Settings:\nSettings ‚Üí Personalization ‚Üí Copilot ‚Üí Disable Via Group Policy (full removal):\nWin + R ‚Üí gpedit.msc User Configuration ‚Üí Administrative Templates ‚Üí Windows Components ‚Üí Windows Copilot Double-click \u0026ldquo;Turn off Windows Copilot\u0026rdquo;, select Enabled, hit Apply.\nDisable SuperFetch (SysMain) Preloads apps into RAM ‚Äî wastes resources in VMs.\nVia Services:\nWin + R ‚Üí services.msc Find SysMain ‚Üí Properties ‚Üí Startup type: Disabled ‚Üí Stop.\nVia Command Line:\nsc stop \u0026#34;SysMain\u0026#34; \u0026amp; sc config \u0026#34;SysMain\u0026#34; start=disabled Disable Visual Effects Search ‚Üí \u0026#34;Adjust the appearance and performance of Windows\u0026#34; Select ‚Üí \u0026#34;Adjust for best performance\u0026#34; Clean Up Startup Programs Via Task Manager:\nCtrl + Shift + Esc ‚Üí Startup tab ‚Üí Disable unnecessary items Reduce Tracking \u0026amp; Telemetry Disable Activity History:\nSettings ‚Üí Privacy \u0026amp; Security ‚Üí Activity History ‚Üí Uncheck \u0026#34;Store my activity history\u0026#34; Disable Speech \u0026amp; Diagnostics:\nSettings ‚Üí Privacy \u0026amp; Security ‚Üí Speech ‚Üí Turn off Settings ‚Üí Privacy \u0026amp; Security ‚Üí Diagnostics \u0026amp; Feedback ‚Üí Disable optional data Clean Start Menu:\nSettings ‚Üí Personalization ‚Üí Start ‚Üí Turn off recommendations You\u0026rsquo;re Done Congratulations! You now have a properly configured, optimized Windows 11 VM that should run significantly better than a default installation. It won\u0026rsquo;t match bare metal performance, but it\u0026rsquo;ll be respectable enough for most tasks. A few final tips:\nSnapshots are your friend: Take one now before you install anything else Allocate RAM wisely: Windows 11 wants at least 4GB, but 8GB is comfortable CPU cores: Give it at least 2 cores, 4 is better if you can spare them Dynamic memory: If using the balloon driver, you can configure memory to expand/contract based on need Now go forth and use Windows when you absolutely have to, but from the comfort of your Linux system!\nüí° Remember: If anything breaks, you have snapshots. Right? \u0026hellip;Right? ","permalink":"https://0xblogs.ashishkus.com/posts/2a597044be8d0d1de091834211c55707/","summary":"\u003ch1 id=\"setting-up-a-windows-11-vm-with-virt-manager-without-losing-your-mind\"\u003eSetting Up a Windows 11 VM with virt-manager (Without Losing Your Mind)\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSo you want to run Windows 11 in a virtual machine on Linux? Excellent choice! Whether you need it for testing, running that one annoying Windows-only app, or just want to see what Microsoft is up to these days, this guide will walk you through creating a properly optimized Windows 11 VM using virt-manager.\u003c/p\u003e\n\u003cp\u003eThe beauty of what we are doing today is that we\u0026rsquo;re going to use VirtIO drivers (which are way faster than the default emulated hardware) and add some Hyper-V enlightenments to make Windows think it\u0026rsquo;s running on native Microsoft hypervisor technology. This makes everything smoother and faster.\u003c/p\u003e","title":"Setting Up a Windows 11 VM with virt-manager (Without Losing Your Mind)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn‚Äôt help but shed a few tears ‚Äî a mix of relief, pride, and sheer satisfaction.\n","permalink":"https://0xblogs.ashishkus.com/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need to be PRO."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"https://0xblogs.ashishkus.com/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"https://0xblogs.ashishkus.com/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom‚Äîyou have a fully functioning desktop environment, office suite, and media apps. It‚Äôs a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It‚Äôs the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle‚Äîbut not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It‚Äôs a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It‚Äôs rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It‚Äôs bleeding edge‚Äîand while that means you sometimes gotta troubleshoot, that‚Äôs part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu‚Äôs package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It‚Äôs a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it‚Äôs probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"https://0xblogs.ashishkus.com/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"}]