[{"content":"Container Lifecycle Run a container from an image\ndocker run [OPTIONS] IMAGE [COMMAND] Run container in background\ndocker run -d IMAGE Run with interactive terminal\ndocker run -it IMAGE Run with custom name\ndocker run --name NAME IMAGE Run with auto-remove on exit\ndocker run --rm IMAGE Create container without starting\ndocker create IMAGE Start stopped container\ndocker start CONTAINER Stop running container\ndocker stop CONTAINER Restart container\ndocker restart CONTAINER Pause container processes\ndocker pause CONTAINER Unpause container\ndocker unpause CONTAINER Kill container immediately\ndocker kill CONTAINER Remove stopped container\ndocker rm CONTAINER Force remove running container\ndocker rm -f CONTAINER Wait for container to stop\ndocker wait CONTAINER Rename container\ndocker rename OLD NEW Update container resources\ndocker update [OPTIONS] CONTAINER Container Information List running containers\ndocker ps List all containers\ndocker ps -a Show container logs\ndocker logs CONTAINER Follow log output\ndocker logs -f CONTAINER Show container processes\ndocker top CONTAINER Show container stats\ndocker stats Show detailed container info\ndocker inspect CONTAINER Show container port mappings\ndocker port CONTAINER Show filesystem changes\ndocker diff CONTAINER Container Interaction Execute command in container\ndocker exec CONTAINER COMMAND Open interactive shell\ndocker exec -it CONTAINER sh Attach to running container\ndocker attach CONTAINER Copy files from container\ndocker cp CONTAINER:SRC DEST Copy files to container\ndocker cp SRC CONTAINER:DEST Export container filesystem\ndocker export CONTAINER \u0026gt; file.tar Create image from container\ndocker commit CONTAINER IMAGE Image Management List images\ndocker images List all images including dangling\ndocker images -a Pull image from registry\ndocker pull IMAGE[:TAG] Push image to registry\ndocker push IMAGE[:TAG] Remove image\ndocker rmi IMAGE Force remove image\ndocker rmi -f IMAGE Tag image\ndocker tag SOURCE TARGET Show image history\ndocker history IMAGE Show detailed image info\ndocker inspect IMAGE Search Docker Hub\ndocker search TERM Save image to tar\ndocker save IMAGE \u0026gt; file.tar Load image from tar\ndocker load \u0026lt; file.tar Import filesystem as image\ndocker import file.tar IMAGE Remove unused images\ndocker image prune Remove all unused images\ndocker image prune -a Image Building Build image from Dockerfile\ndocker build -t NAME PATH Build with tag\ndocker build -t NAME:TAG PATH Build with custom Dockerfile\ndocker build -f Dockerfile -t NAME PATH Build without cache\ndocker build --no-cache -t NAME PATH Build with build args\ndocker build --build-arg KEY=VALUE -t NAME PATH Build specific stage\ndocker build --target STAGE -t NAME PATH Build for platform\ndocker build --platform PLATFORM -t NAME PATH Volume Management Create volume\ndocker volume create NAME List volumes\ndocker volume ls Show volume details\ndocker volume inspect NAME Remove volume\ndocker volume rm NAME Remove unused volumes\ndocker volume prune Mount named volume\ndocker run -v VOLUME:PATH IMAGE Mount bind volume\ndocker run -v HOST_PATH:CONTAINER_PATH IMAGE Mount read-only volume\ndocker run -v VOLUME:PATH:ro IMAGE Create anonymous volume\ndocker run -v PATH IMAGE Mount with volume driver\ndocker run --mount type=volume,source=VOL,target=PATH IMAGE Network Management List networks\ndocker network ls Create network\ndocker network create NAME Create with driver\ndocker network create --driver DRIVER NAME Create with subnet\ndocker network create --subnet CIDR NAME Create with gateway\ndocker network create --gateway IP NAME Show network details\ndocker network inspect NAME Remove network\ndocker network rm NAME Remove unused networks\ndocker network prune Connect container to network\ndocker network connect NETWORK CONTAINER Connect with IP\ndocker network connect --ip IP NETWORK CONTAINER Connect with alias\ndocker network connect --alias ALIAS NETWORK CONTAINER Disconnect container\ndocker network disconnect NETWORK CONTAINER Run on specific network\ndocker run --network NAME IMAGE Run with network alias\ndocker run --network-alias ALIAS IMAGE Registry \u0026amp; Authentication Login to registry\ndocker login Login to custom registry\ndocker login REGISTRY Login with credentials\ndocker login -u USER -p PASS Logout from registry\ndocker logout Pull from custom registry\ndocker pull REGISTRY/IMAGE Push to custom registry\ndocker push REGISTRY/IMAGE Tag for registry\ndocker tag IMAGE REGISTRY/IMAGE:TAG Docker Compose Start services\ndocker-compose up Start in background\ndocker-compose up -d Stop services\ndocker-compose down Stop and remove volumes\ndocker-compose down -v Build services\ndocker-compose build Build without cache\ndocker-compose build --no-cache List services\ndocker-compose ps View logs\ndocker-compose logs Follow logs\ndocker-compose logs -f Execute command\ndocker-compose exec SERVICE COMMAND Run one-off command\ndocker-compose run SERVICE COMMAND Start services\ndocker-compose start Stop services\ndocker-compose stop Restart services\ndocker-compose restart Pause services\ndocker-compose pause Unpause services\ndocker-compose unpause Remove stopped containers\ndocker-compose rm Pull service images\ndocker-compose pull Push service images\ndocker-compose push Validate compose file\ndocker-compose config Scale services\ndocker-compose up --scale SERVICE=NUM Show running processes\ndocker-compose top System Management Show Docker info\ndocker info Show Docker version\ndocker version Show disk usage\ndocker system df Remove unused data\ndocker system prune Remove all unused data\ndocker system prune -a Remove with volumes\ndocker system prune --volumes Show real-time events\ndocker events Filter events\ndocker events --filter event=TYPE Events since time\ndocker events --since TIME Swarm Management Initialize swarm\ndocker swarm init Initialize with advertise address\ndocker swarm init --advertise-addr IP Join as worker\ndocker swarm join --token TOKEN IP:PORT Join as manager\ndocker swarm join --token TOKEN IP:PORT Leave swarm\ndocker swarm leave Force leave swarm\ndocker swarm leave -f Show join token\ndocker swarm join-token worker Show manager token\ndocker swarm join-token manager Update swarm\ndocker swarm update Unlock swarm\ndocker swarm unlock Show unlock key\ndocker swarm unlock-key Stack Management Deploy stack\ndocker stack deploy -c FILE STACK List stacks\ndocker stack ls List stack services\ndocker stack services STACK List stack tasks\ndocker stack ps STACK Remove stack\ndocker stack rm STACK Service Management Create service\ndocker service create IMAGE List services\ndocker service ls Show service details\ndocker service inspect SERVICE Show service logs\ndocker service logs SERVICE List service tasks\ndocker service ps SERVICE Scale service\ndocker service scale SERVICE=NUM Update service\ndocker service update SERVICE Update image\ndocker service update --image IMAGE SERVICE Remove service\ndocker service rm SERVICE Rollback service\ndocker service rollback SERVICE Node Management List nodes\ndocker node ls Show node details\ndocker node inspect NODE Update node\ndocker node update NODE Promote to manager\ndocker node promote NODE Demote to worker\ndocker node demote NODE Remove node\ndocker node rm NODE List node tasks\ndocker node ps NODE Set node availability\ndocker node update --availability active|pause|drain NODE Secret Management Create secret from file\ndocker secret create NAME FILE Create secret from stdin\necho \u0026#34;data\u0026#34; | docker secret create NAME - List secrets\ndocker secret ls Show secret details\ndocker secret inspect NAME Remove secret\ndocker secret rm NAME Config Management Create config from file\ndocker config create NAME FILE Create config from stdin\necho \u0026#34;data\u0026#34; | docker config create NAME - List configs\ndocker config ls Show config details\ndocker config inspect NAME Remove config\ndocker config rm NAME Plugin Management List plugins\ndocker plugin ls Install plugin\ndocker plugin install PLUGIN Enable plugin\ndocker plugin enable PLUGIN Disable plugin\ndocker plugin disable PLUGIN Remove plugin\ndocker plugin rm PLUGIN Show plugin details\ndocker plugin inspect PLUGIN Upgrade plugin\ndocker plugin upgrade PLUGIN Push plugin\ndocker plugin push PLUGIN Create plugin\ndocker plugin create PLUGIN PATH Configure plugin\ndocker plugin set PLUGIN KEY=VALUE Context Management List contexts\ndocker context ls Create context\ndocker context create NAME Use context\ndocker context use NAME Show context details\ndocker context inspect NAME Update context\ndocker context update NAME Remove context\ndocker context rm NAME Export context\ndocker context export NAME Import context\ndocker context import NAME FILE Builder Management List builders\ndocker builder ls Prune build cache\ndocker builder prune Show builder disk usage\ndocker builder du Trust \u0026amp; Content Trust Sign and push image\ndocker trust sign IMAGE:TAG Show trust data\ndocker trust inspect IMAGE Revoke trust\ndocker trust revoke IMAGE:TAG Generate trust key\ndocker trust key generate NAME Load trust key\ndocker trust key load FILE Add signer\ndocker trust signer add --key FILE SIGNER REPO Remove signer\ndocker trust signer remove SIGNER REPO Checkpoint Management Create checkpoint\ndocker checkpoint create CONTAINER NAME List checkpoints\ndocker checkpoint ls CONTAINER Remove checkpoint\ndocker checkpoint rm CONTAINER NAME Manifest Management Inspect manifest\ndocker manifest inspect IMAGE Create manifest\ndocker manifest create LIST IMAGE... Push manifest\ndocker manifest push LIST Annotate manifest\ndocker manifest annotate LIST IMAGE --os OS --arch ARCH Container Cleanup Remove all stopped containers\ndocker container prune Remove containers older than\ndocker container prune --filter \u0026#34;until=24h\u0026#34; Remove with label filter\ndocker container prune --filter \u0026#34;label=KEY\u0026#34; Image Cleanup Remove dangling images\ndocker image prune Remove all unused images\ndocker image prune -a Remove images older than\ndocker image prune -a --filter \u0026#34;until=24h\u0026#34; Volume Cleanup Remove all unused volumes\ndocker volume prune Remove with filter\ndocker volume prune --filter \u0026#34;label=KEY\u0026#34; Network Cleanup Remove all unused networks\ndocker network prune Remove with filter\ndocker network prune --filter \u0026#34;until=24h\u0026#34; Common Run Options Run with port mapping\ndocker run -p HOST:CONTAINER IMAGE Run with environment variable\ndocker run -e KEY=VALUE IMAGE Run with multiple env vars\ndocker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Run with env file\ndocker run --env-file FILE IMAGE Run with working directory\ndocker run -w PATH IMAGE Run as specific user\ndocker run -u USER IMAGE Run with hostname\ndocker run -h HOSTNAME IMAGE Run with CPU limit\ndocker run --cpus NUM IMAGE Run with memory limit\ndocker run -m SIZE IMAGE Run with privileged mode\ndocker run --privileged IMAGE Run with cap add\ndocker run --cap-add CAPABILITY IMAGE Run with cap drop\ndocker run --cap-drop CAPABILITY IMAGE Run with device\ndocker run --device PATH IMAGE Run with restart policy\ndocker run --restart=always IMAGE Run with label\ndocker run -l KEY=VALUE IMAGE Run with link\ndocker run --link CONTAINER IMAGE Run with DNS\ndocker run --dns IP IMAGE Run with add-host\ndocker run --add-host HOST:IP IMAGE Run with pid mode\ndocker run --pid host IMAGE Run with IPC mode\ndocker run --ipc host IMAGE Tip: Use docker COMMAND --help for detailed options and examples.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003cp\u003eRun a container from an image\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun container in background\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with interactive terminal\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -it IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with custom name\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run --name NAME IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with auto-remove on exit\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run --rm IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreate container without starting\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker create IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStart stopped container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker start CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStop running container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker stop CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRestart container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker restart CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePause container processes\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Cheat Sheet Container Lifecycle Run a container from an image\ndocker run [OPTIONS] IMAGE [COMMAND] Run container in background\ndocker run -d IMAGE Run with interactive terminal\ndocker run -it IMAGE Run with custom name\ndocker run --name NAME IMAGE Run with auto-remove on exit\ndocker run --rm IMAGE Create container without starting\ndocker create IMAGE Start stopped container\ndocker start CONTAINER Stop running container\ndocker stop CONTAINER Restart container\ndocker restart CONTAINER Pause container processes\ndocker pause CONTAINER Unpause container\ndocker unpause CONTAINER Kill container immediately\ndocker kill CONTAINER Remove stopped container\ndocker rm CONTAINER Force remove running container\ndocker rm -f CONTAINER Wait for container to stop\ndocker wait CONTAINER Rename container\ndocker rename OLD NEW Update container resources\ndocker update [OPTIONS] CONTAINER Container Information List running containers\ndocker ps List all containers\ndocker ps -a Show container logs\ndocker logs CONTAINER Follow log output\ndocker logs -f CONTAINER Show container processes\ndocker top CONTAINER Show container stats\ndocker stats Show detailed container info\ndocker inspect CONTAINER Show container port mappings\ndocker port CONTAINER Show filesystem changes\ndocker diff CONTAINER Container Interaction Execute command in container\ndocker exec CONTAINER COMMAND Open interactive shell\ndocker exec -it CONTAINER sh Attach to running container\ndocker attach CONTAINER Copy files from container\ndocker cp CONTAINER:SRC DEST Copy files to container\ndocker cp SRC CONTAINER:DEST Export container filesystem\ndocker export CONTAINER \u0026gt; file.tar Create image from container\ndocker commit CONTAINER IMAGE Image Management List images\ndocker images List all images including dangling\ndocker images -a Pull image from registry\ndocker pull IMAGE[:TAG] Push image to registry\ndocker push IMAGE[:TAG] Remove image\ndocker rmi IMAGE Force remove image\ndocker rmi -f IMAGE Tag image\ndocker tag SOURCE TARGET Show image history\ndocker history IMAGE Show detailed image info\ndocker inspect IMAGE Search Docker Hub\ndocker search TERM Save image to tar\ndocker save IMAGE \u0026gt; file.tar Load image from tar\ndocker load \u0026lt; file.tar Import filesystem as image\ndocker import file.tar IMAGE Remove unused images\ndocker image prune Remove all unused images\ndocker image prune -a Image Building Build image from Dockerfile\ndocker build -t NAME PATH Build with tag\ndocker build -t NAME:TAG PATH Build with custom Dockerfile\ndocker build -f Dockerfile -t NAME PATH Build without cache\ndocker build --no-cache -t NAME PATH Build with build args\ndocker build --build-arg KEY=VALUE -t NAME PATH Build specific stage\ndocker build --target STAGE -t NAME PATH Build for platform\ndocker build --platform PLATFORM -t NAME PATH Volume Management Create volume\ndocker volume create NAME List volumes\ndocker volume ls Show volume details\ndocker volume inspect NAME Remove volume\ndocker volume rm NAME Remove unused volumes\ndocker volume prune Mount named volume\ndocker run -v VOLUME:PATH IMAGE Mount bind volume\ndocker run -v HOST_PATH:CONTAINER_PATH IMAGE Mount read-only volume\ndocker run -v VOLUME:PATH:ro IMAGE Create anonymous volume\ndocker run -v PATH IMAGE Mount with volume driver\ndocker run --mount type=volume,source=VOL,target=PATH IMAGE Network Management List networks\ndocker network ls Create network\ndocker network create NAME Create with driver\ndocker network create --driver DRIVER NAME Create with subnet\ndocker network create --subnet CIDR NAME Create with gateway\ndocker network create --gateway IP NAME Show network details\ndocker network inspect NAME Remove network\ndocker network rm NAME Remove unused networks\ndocker network prune Connect container to network\ndocker network connect NETWORK CONTAINER Connect with IP\ndocker network connect --ip IP NETWORK CONTAINER Connect with alias\ndocker network connect --alias ALIAS NETWORK CONTAINER Disconnect container\ndocker network disconnect NETWORK CONTAINER Run on specific network\ndocker run --network NAME IMAGE Run with network alias\ndocker run --network-alias ALIAS IMAGE Registry \u0026amp; Authentication Login to registry\ndocker login Login to custom registry\ndocker login REGISTRY Login with credentials\ndocker login -u USER -p PASS Logout from registry\ndocker logout Pull from custom registry\ndocker pull REGISTRY/IMAGE Push to custom registry\ndocker push REGISTRY/IMAGE Tag for registry\ndocker tag IMAGE REGISTRY/IMAGE:TAG Docker Compose Start services\ndocker-compose up Start in background\ndocker-compose up -d Stop services\ndocker-compose down Stop and remove volumes\ndocker-compose down -v Build services\ndocker-compose build Build without cache\ndocker-compose build --no-cache List services\ndocker-compose ps View logs\ndocker-compose logs Follow logs\ndocker-compose logs -f Execute command\ndocker-compose exec SERVICE COMMAND Run one-off command\ndocker-compose run SERVICE COMMAND Start services\ndocker-compose start Stop services\ndocker-compose stop Restart services\ndocker-compose restart Pause services\ndocker-compose pause Unpause services\ndocker-compose unpause Remove stopped containers\ndocker-compose rm Pull service images\ndocker-compose pull Push service images\ndocker-compose push Validate compose file\ndocker-compose config Scale services\ndocker-compose up --scale SERVICE=NUM Show running processes\ndocker-compose top System Management Show Docker info\ndocker info Show Docker version\ndocker version Show disk usage\ndocker system df Remove unused data\ndocker system prune Remove all unused data\ndocker system prune -a Remove with volumes\ndocker system prune --volumes Show real-time events\ndocker events Filter events\ndocker events --filter event=TYPE Events since time\ndocker events --since TIME Swarm Management Initialize swarm\ndocker swarm init Initialize with advertise address\ndocker swarm init --advertise-addr IP Join as worker\ndocker swarm join --token TOKEN IP:PORT Join as manager\ndocker swarm join --token TOKEN IP:PORT Leave swarm\ndocker swarm leave Force leave swarm\ndocker swarm leave -f Show join token\ndocker swarm join-token worker Show manager token\ndocker swarm join-token manager Update swarm\ndocker swarm update Unlock swarm\ndocker swarm unlock Show unlock key\ndocker swarm unlock-key Stack Management Deploy stack\ndocker stack deploy -c FILE STACK List stacks\ndocker stack ls List stack services\ndocker stack services STACK List stack tasks\ndocker stack ps STACK Remove stack\ndocker stack rm STACK Service Management Create service\ndocker service create IMAGE List services\ndocker service ls Show service details\ndocker service inspect SERVICE Show service logs\ndocker service logs SERVICE List service tasks\ndocker service ps SERVICE Scale service\ndocker service scale SERVICE=NUM Update service\ndocker service update SERVICE Update image\ndocker service update --image IMAGE SERVICE Remove service\ndocker service rm SERVICE Rollback service\ndocker service rollback SERVICE Node Management List nodes\ndocker node ls Show node details\ndocker node inspect NODE Update node\ndocker node update NODE Promote to manager\ndocker node promote NODE Demote to worker\ndocker node demote NODE Remove node\ndocker node rm NODE List node tasks\ndocker node ps NODE Set node availability\ndocker node update --availability active|pause|drain NODE Secret Management Create secret from file\ndocker secret create NAME FILE Create secret from stdin\necho \u0026#34;data\u0026#34; | docker secret create NAME - List secrets\ndocker secret ls Show secret details\ndocker secret inspect NAME Remove secret\ndocker secret rm NAME Config Management Create config from file\ndocker config create NAME FILE Create config from stdin\necho \u0026#34;data\u0026#34; | docker config create NAME - List configs\ndocker config ls Show config details\ndocker config inspect NAME Remove config\ndocker config rm NAME Plugin Management List plugins\ndocker plugin ls Install plugin\ndocker plugin install PLUGIN Enable plugin\ndocker plugin enable PLUGIN Disable plugin\ndocker plugin disable PLUGIN Remove plugin\ndocker plugin rm PLUGIN Show plugin details\ndocker plugin inspect PLUGIN Upgrade plugin\ndocker plugin upgrade PLUGIN Push plugin\ndocker plugin push PLUGIN Create plugin\ndocker plugin create PLUGIN PATH Configure plugin\ndocker plugin set PLUGIN KEY=VALUE Context Management List contexts\ndocker context ls Create context\ndocker context create NAME Use context\ndocker context use NAME Show context details\ndocker context inspect NAME Update context\ndocker context update NAME Remove context\ndocker context rm NAME Export context\ndocker context export NAME Import context\ndocker context import NAME FILE Builder Management List builders\ndocker builder ls Prune build cache\ndocker builder prune Show builder disk usage\ndocker builder du Trust \u0026amp; Content Trust Sign and push image\ndocker trust sign IMAGE:TAG Show trust data\ndocker trust inspect IMAGE Revoke trust\ndocker trust revoke IMAGE:TAG Generate trust key\ndocker trust key generate NAME Load trust key\ndocker trust key load FILE Add signer\ndocker trust signer add --key FILE SIGNER REPO Remove signer\ndocker trust signer remove SIGNER REPO Checkpoint Management Create checkpoint\ndocker checkpoint create CONTAINER NAME List checkpoints\ndocker checkpoint ls CONTAINER Remove checkpoint\ndocker checkpoint rm CONTAINER NAME Manifest Management Inspect manifest\ndocker manifest inspect IMAGE Create manifest\ndocker manifest create LIST IMAGE... Push manifest\ndocker manifest push LIST Annotate manifest\ndocker manifest annotate LIST IMAGE --os OS --arch ARCH Container Cleanup Remove all stopped containers\ndocker container prune Remove containers older than\ndocker container prune --filter \u0026#34;until=24h\u0026#34; Remove with label filter\ndocker container prune --filter \u0026#34;label=KEY\u0026#34; Image Cleanup Remove dangling images\ndocker image prune Remove all unused images\ndocker image prune -a Remove images older than\ndocker image prune -a --filter \u0026#34;until=24h\u0026#34; Volume Cleanup Remove all unused volumes\ndocker volume prune Remove with filter\ndocker volume prune --filter \u0026#34;label=KEY\u0026#34; Network Cleanup Remove all unused networks\ndocker network prune Remove with filter\ndocker network prune --filter \u0026#34;until=24h\u0026#34; Common Run Options Run with port mapping\ndocker run -p HOST:CONTAINER IMAGE Run with environment variable\ndocker run -e KEY=VALUE IMAGE Run with multiple env vars\ndocker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Run with env file\ndocker run --env-file FILE IMAGE Run with working directory\ndocker run -w PATH IMAGE Run as specific user\ndocker run -u USER IMAGE Run with hostname\ndocker run -h HOSTNAME IMAGE Run with CPU limit\ndocker run --cpus NUM IMAGE Run with memory limit\ndocker run -m SIZE IMAGE Run with privileged mode\ndocker run --privileged IMAGE Run with cap add\ndocker run --cap-add CAPABILITY IMAGE Run with cap drop\ndocker run --cap-drop CAPABILITY IMAGE Run with device\ndocker run --device PATH IMAGE Run with restart policy\ndocker run --restart=always IMAGE Run with label\ndocker run -l KEY=VALUE IMAGE Run with link\ndocker run --link CONTAINER IMAGE Run with DNS\ndocker run --dns IP IMAGE Run with add-host\ndocker run --add-host HOST:IP IMAGE Run with pid mode\ndocker run --pid host IMAGE Run with IPC mode\ndocker run --ipc host IMAGE Tip: Use docker COMMAND --help for detailed options and examples.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"cheat-sheet\"\u003eCheat Sheet\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003cp\u003eRun a container from an image\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun container in background\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with interactive terminal\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -it IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with custom name\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run --name NAME IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun with auto-remove on exit\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run --rm IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreate container without starting\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker create IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStart stopped container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker start CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStop running container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker stop CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRestart container\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker restart CONTAINER\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePause container processes\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide A comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management List plugins docker plugin ls Lists all installed Docker plugins. Plugins extend Docker\u0026rsquo;s core functionality, often for volume or network drivers.\nInstall plugin docker plugin install PLUGIN Downloads and installs a plugin from a registry. The plugin must be explicitly enabled after installation.\nEnable plugin docker plugin enable PLUGIN Starts a stopped plugin. A plugin must be in an enabled state to function and provide its extended features.\nDisable plugin docker plugin disable PLUGIN Stops a running plugin. Disabling a plugin removes its functionality from the Docker engine.\nRemove plugin docker plugin rm PLUGIN Removes an installed plugin from the host. The plugin must be disabled before it can be removed.\nShow plugin details docker plugin inspect PLUGIN Returns detailed JSON information about an installed plugin. This includes its configuration, settings, and network/volume properties.\nPlugin Management List plugins\ndocker plugin ls Install plugin\ndocker plugin install PLUGIN Enable plugin\ndocker plugin enable PLUGIN Disable plugin\ndocker plugin disable PLUGIN Remove plugin\ndocker plugin rm PLUGIN Show plugin details\ndocker plugin inspect PLUGIN Upgrade plugin\ndocker plugin upgrade PLUGIN Push plugin\ndocker plugin push PLUGIN Create plugin\ndocker plugin create PLUGIN PATH Configure plugin\ndocker plugin set PLUGIN KEY=VALUE Context Management List contexts\ndocker context ls Create context\ndocker context create NAME Use context\ndocker context use NAME Show context details\ndocker context inspect NAME Update context\ndocker context update NAME Remove context\ndocker context rm NAME Export context\ndocker context export NAME Import context\ndocker context import NAME FILE Builder Management List builders\ndocker builder ls Prune build cache\ndocker builder prune Show builder disk usage\ndocker builder du Trust \u0026amp; Content Trust Sign and push image\ndocker trust sign IMAGE:TAG Show trust data\ndocker trust inspect IMAGE Revoke trust\ndocker trust revoke IMAGE:TAG Generate trust key\ndocker trust key generate NAME Load trust key\ndocker trust key load FILE Add signer\ndocker trust signer add --key FILE SIGNER REPO Remove signer\ndocker trust signer remove SIGNER REPO Checkpoint Management Create checkpoint\ndocker checkpoint create CONTAINER NAME List checkpoints\ndocker checkpoint ls CONTAINER Remove checkpoint\ndocker checkpoint rm CONTAINER NAME Manifest Management Inspect manifest\ndocker manifest inspect IMAGE Create manifest\ndocker manifest create LIST IMAGE... Push manifest\ndocker manifest push LIST Annotate manifest\ndocker manifest annotate LIST IMAGE --os OS --arch ARCH Container Cleanup Remove all stopped containers\ndocker container prune Remove containers older than\ndocker container prune --filter \u0026#34;until=24h\u0026#34; Remove with label filter\ndocker container prune --filter \u0026#34;label=KEY\u0026#34; Image Cleanup Remove dangling images\ndocker image prune Remove all unused images\ndocker image prune -a Remove images older than\ndocker image prune -a --filter \u0026#34;until=24h\u0026#34; Volume Cleanup Remove all unused volumes\ndocker volume prune Remove with filter\ndocker volume prune --filter \u0026#34;label=KEY\u0026#34; Network Cleanup Remove all unused networks\ndocker network prune Remove with filter\ndocker network prune --filter \u0026#34;until=24h\u0026#34; Common Run Options Run with port mapping\ndocker run -p HOST:CONTAINER IMAGE Run with environment variable\ndocker run -e KEY=VALUE IMAGE Run with multiple env vars\ndocker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Run with env file\ndocker run --env-file FILE IMAGE Run with working directory\ndocker run -w PATH IMAGE Run as specific user\ndocker run -u USER IMAGE Run with hostname\ndocker run -h HOSTNAME IMAGE Run with CPU limit\ndocker run --cpus NUM IMAGE Run with memory limit\ndocker run -m SIZE IMAGE Run with privileged mode\ndocker run --privileged IMAGE Run with cap add\ndocker run --cap-add CAPABILITY IMAGE Run with cap drop\ndocker run --cap-drop CAPABILITY IMAGE Run with device\ndocker run --device PATH IMAGE Run with restart policy\ndocker run --restart=always IMAGE Run with label\ndocker run -l KEY=VALUE IMAGE Run with link\ndocker run --link CONTAINER IMAGE Run with DNS\ndocker run --dns IP IMAGE Run with add-host\ndocker run --add-host HOST:IP IMAGE Run with pid mode\ndocker run --pid host IMAGE Run with IPC mode\ndocker run --ipc host IMAGE Tip: Use docker COMMAND --help for detailed options and examples.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"a-comprehensive-reference-for-the-docker-command-line-interface-cli-covering-everything-from-basic-container-management-to-advanced-orchestration-operations\"\u003eA comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations\u003c/h2\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide A comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"a-comprehensive-reference-for-the-docker-command-line-interface-cli-covering-everything-from-basic-container-management-to-advanced-orchestration-operations\"\u003eA comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations\u003c/h2\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide A comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"a-comprehensive-reference-for-the-docker-command-line-interface-cli-covering-everything-from-basic-container-management-to-advanced-orchestration-operations\"\u003eA comprehensive reference for the Docker Command Line Interface (CLI), covering everything from basic container management to advanced orchestration operations\u003c/h2\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide ## Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\n## Container Lifecycle\n\u003c/summary\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003c/summary\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003c/summary\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide ## Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003cdetails\u003e\n\u003csummary\u003e ## Container Lifecycle \u003c/summary\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nThis comprehensive Docker CLI cheat sheet covers all essential commands for container management, orchestration, networking, security, and system maintenance. Keep it handy for quick reference!\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CheatSheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need to be PRO."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"},{"content":"Docker CLI Cheat Sheet: Complete Reference Guide Container Lifecycle Run a container from an image docker run [OPTIONS] IMAGE [COMMAND] Creates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\nRun container in background docker run -d IMAGE Runs the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\nRun with interactive terminal docker run -it IMAGE Runs the container with an interactive terminal (-i) and allocates a pseudo-TTY (-t). This is essential for interacting with the container, like running an interactive shell such as bash.\nRun with custom name docker run --name NAME IMAGE Assigns a custom, human-readable NAME to the container instead of a randomly generated one. This makes it easier to reference the container in other Docker commands.\nRun with auto-remove on exit docker run --rm IMAGE Automatically removes the container\u0026rsquo;s filesystem when it exits. This is useful for running temporary or short-lived jobs to keep your system clean.\nCreate container without starting docker create IMAGE Creates a container from an image but does not start it. This allows you to configure the container or volume mounts before starting it manually with docker start.\nStart stopped container docker start CONTAINER Starts one or more existing, stopped containers. This is used to resume a container that was previously stopped without having to re-create it.\nStop running container docker stop CONTAINER Gracefully shuts down a running container by sending a SIGTERM signal. Docker waits for a default timeout (usually 10 seconds) for the container to exit before sending a SIGKILL.\nRestart container docker restart CONTAINER Stops a running container and then starts it again. This is a convenient way to apply changes or simply recycle a container\u0026rsquo;s processes.\nPause container processes docker pause CONTAINER Suspends all processes inside one or more running containers. It achieves this by using the cgroups freezer to temporarily stop the container\u0026rsquo;s execution.\nUnpause container docker unpause CONTAINER Resumes all processes inside a paused container. This reverses the effect of the docker pause command, allowing the container\u0026rsquo;s processes to continue execution.\nKill container immediately docker kill CONTAINER Immediately stops a container\u0026rsquo;s execution by sending a SIGKILL signal. This should be used when a container is unresponsive to the gentler docker stop command.\nRemove stopped container docker rm CONTAINER Removes one or more stopped containers. It cannot remove a running container unless the -f (force) flag is used.\nForce remove running container docker rm -f CONTAINER Forces the removal of a container, even if it is currently running. This is an abrupt removal that is equivalent to running docker kill followed by docker rm.\nWait for container to stop docker wait CONTAINER Blocks the current shell execution until one or more containers stop. It then prints the container\u0026rsquo;s exit codes, which is useful for scripting sequential operations.\nRename container docker rename OLD NEW Changes the custom name of an existing container from OLD to NEW. This is a simple utility command for maintaining better organization of your containers.\nUpdate container resources docker update [OPTIONS] CONTAINER Allows for dynamic configuration updates to a running container\u0026rsquo;s resource constraints. You can use it to change limits like memory (-m) or CPU shares.\nContainer Information List running containers docker ps Displays a list of all currently running containers. It provides essential information like Container ID, Image, Status, and Ports.\nList all containers docker ps -a Displays a list of all containers, including those that are stopped or exited. This is the full inventory of containers present on your Docker host.\nShow container logs docker logs CONTAINER Retrieves the standard output (stdout) and standard error (stderr) logs for a container. It shows the history of what the container\u0026rsquo;s main process has written.\nFollow log output docker logs -f CONTAINER Streams the log output from a container in real-time. This is useful for monitoring the activity of a running service.\nShow container processes docker top CONTAINER Displays the running processes within a container. It\u0026rsquo;s similar to the Linux top command but scoped to the container\u0026rsquo;s environment.\nShow container stats docker stats Displays a live stream of resource usage statistics for all running containers. It includes CPU, memory usage, network I/O, and disk I/O.\nShow detailed container info docker inspect CONTAINER Returns low-level, detailed JSON configuration information about a container. This is a powerful command used for debugging and viewing all configuration details.\nShow container port mappings docker port CONTAINER Displays the public-facing port mapping for a container to its private ports. It quickly shows you which HOST ports are mapped to which CONTAINER ports.\nShow filesystem changes docker diff CONTAINER Displays the changes made to the container\u0026rsquo;s filesystem since it was created. It indicates files that have been added (A), deleted (D), or modified (C).\nContainer Interaction Execute command in container docker exec CONTAINER COMMAND Runs a new command inside an already running container. The original process of the container remains running.\nOpen interactive shell docker exec -it CONTAINER sh Executes an interactive shell (like sh or bash) inside the container. This is the primary way to get a command prompt and debug inside a running service.\nAttach to running container docker attach CONTAINER Attaches your local standard input, output, and error streams to a running container\u0026rsquo;s main process. This is generally used to view the real-time output or interact with a process that started interactively.\nCopy files from container docker cp CONTAINER:SRC DEST Copies files or folders from the container\u0026rsquo;s filesystem at SRC to the local machine at DEST. This is a simple way to pull data out of a container.\nCopy files to container docker cp SRC CONTAINER:DEST Copies files or folders from the local machine at SRC into the container\u0026rsquo;s filesystem at DEST. This is a quick utility for adding necessary files to a running container.\nExport container filesystem docker export CONTAINER \u0026gt; file.tar Exports the contents of a container\u0026rsquo;s filesystem as a tar archive. It does not include the metadata or base image layers, just a flattened filesystem.\nCreate image from container docker commit CONTAINER IMAGE Creates a new image by capturing the current state of a running or stopped container. This is typically a less-preferred method than using a Dockerfile but is useful for quick saves.\nImage Management List images docker images Displays a list of all locally stored Docker images. It provides the image repository, tag, ID, creation time, and size.\nList all images including dangling docker images -a Shows all local images, including intermediate images used during builds and \u0026ldquo;dangling\u0026rdquo; images (untagged, unused layers). This helps in identifying and cleaning up all image data.\nPull image from registry docker pull IMAGE[:TAG] Downloads an image and its necessary layers from a registry, typically Docker Hub. If no TAG is specified, it defaults to the :latest tag.\nPush image to registry docker push IMAGE[:TAG] Uploads a local image and its layers to a configured registry (e.g., Docker Hub). The image must be appropriately tagged with the registry path before pushing.\nRemove image docker rmi IMAGE Removes one or more local images. An image cannot be removed if it is currently being used by any container.\nForce remove image docker rmi -f IMAGE Forces the removal of an image, even if it is currently in use by a container. It achieves this by also removing any references to the image.\nTag image docker tag SOURCE TARGET Creates an alias for an image, giving it a new name and/or tag. This is most often used to prepare an image with a full registry path before pushing it.\nShow image history docker history IMAGE Displays the history of an image, showing each layer and the command used to create it. This is useful for understanding how an image was built and its size progression.\nShow detailed image info docker inspect IMAGE Returns low-level, detailed JSON configuration information about an image. It provides metadata, layer history, configuration, and image size details.\nSearch Docker Hub docker search TERM Searches the public Docker Hub registry for images matching a specific TERM. This helps discover publicly available images to use as base images.\nSave image to tar docker save IMAGE \u0026gt; file.tar Saves one or more images into a tar archive. This is used for backing up an image or transferring it to a host that is offline (air-gapped).\nLoad image from tar docker load \u0026lt; file.tar Loads a tar archive from standard input as a Docker image. This is the command used to import an image that was previously saved using docker save.\nImport filesystem as image docker import file.tar IMAGE Creates a new image from the contents of a tarball, using it as a flattened filesystem. Unlike docker load, this creates a single-layer, non-history image.\nRemove unused images docker image prune Removes all dangling images (untagged and unreferenced by any container). This is a basic housekeeping command to free up disk space.\nRemove all unused images docker image prune -a Removes all unused images, including both dangling and non-dangling ones not referenced by any container. Use this with caution as it removes any image not actively in use.\nImage Building Build image from Dockerfile docker build -t NAME PATH Builds a new Docker image using the Dockerfile located in the specified PATH (the build context). The -t flag tags the resulting image with a friendly NAME.\nBuild with tag docker build -t NAME:TAG PATH Builds and tags the image with a specific NAME and TAG. This is the best practice for versioning your images.\nBuild with custom Dockerfile docker build -f Dockerfile -t NAME PATH Specifies an alternative path and file name for the Dockerfile to use for the build. This is necessary when the Dockerfile is not named Dockerfile or is not in the root of the build context.\nBuild without cache docker build --no-cache -t NAME PATH Forces the builder to execute all steps without using any previously cached layers. This is useful when you suspect the cache is stale or you need a clean build.\nBuild with build args docker build --build-arg KEY=VALUE -t NAME PATH Passes a build-time variable to the Dockerfile, which can be referenced via the ARG instruction. This is used to inject dynamic values like versions or credentials during the build.\nBuild specific stage docker build --target STAGE -t NAME PATH Builds only up to the specified STAGE name in a multi-stage Dockerfile. This is essential for debugging intermediate stages or creating smaller \u0026ldquo;builder\u0026rdquo; images.\nBuild for platform docker build --platform PLATFORM -t NAME PATH Specifies the target operating system and architecture for the built image. This is crucial for cross-platform builds, such as building an ARM image on an x86 host.\nVolume Management Create volume docker volume create NAME Creates a new, managed Docker volume with a specific NAME. Volumes are the preferred way to persist data generated by and used by Docker containers.\nList volumes docker volume ls Displays a list of all local Docker volumes. It shows the volume name and the driver being used.\nShow volume details docker volume inspect NAME Returns low-level, detailed JSON configuration information about a volume. This includes the mount point on the host filesystem and the driver used.\nRemove volume docker volume rm NAME Removes one or more specified volumes. A volume can only be removed if it is not currently in use by any container.\nRemove unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is a safe cleanup command for removing orphaned volume data.\nMount named volume docker run -v VOLUME:PATH IMAGE Mounts the named Docker VOLUME to the specified PATH inside the container. This is the standard way to ensure data persistence for a container.\nMount bind volume docker run -v HOST_PATH:CONTAINER_PATH IMAGE Binds a directory from the host machine (HOST_PATH) to a path inside the container (CONTAINER_PATH). This is useful for development, allowing the container to access local source code.\nMount read-only volume docker run -v VOLUME:PATH:ro IMAGE Mounts a volume or bind mount to the container\u0026rsquo;s path and sets it as read-only (:ro). This prevents the container from writing to the mounted location, enhancing security and data integrity.\nCreate anonymous volume docker run -v PATH IMAGE Creates an anonymous volume when no volume name is specified, mounting it to the container\u0026rsquo;s PATH. The volume is managed by Docker but has a long, random ID.\nMount with volume driver docker run --mount type=volume,source=VOL,target=PATH IMAGE The newer and more explicit way to mount a volume, using the --mount flag. This syntax is often preferred as it is more verbose and clearer than the -v shorthand.\nNetwork Management List networks docker network ls Displays a list of all local Docker networks (e.g., bridge, host, none, and custom networks). It shows the network ID, name, and driver.\nCreate network docker network create NAME Creates a new, user-defined network with a specified NAME. Containers on the same user-defined network can resolve each other by name.\nCreate with driver docker network create --driver DRIVER NAME Creates a network using a specific type of network DRIVER (e.g., bridge, overlay). The overlay driver is used for multi-host networking in a Swarm cluster.\nCreate with subnet docker network create --subnet CIDR NAME Creates a network and specifies the IP address range for the network using CIDR notation. This allows for fine-tuning the IP addressing within the network.\nCreate with gateway docker network create --gateway IP NAME Specifies the default gateway IP address for the newly created network. This gives greater control over the networking configuration.\nShow network details docker network inspect NAME Returns low-level, detailed JSON configuration information about a network. It includes the subnet, gateway, driver, and a list of all connected containers.\nRemove network docker network rm NAME Removes one or more specified networks. A network can only be removed if there are no containers currently attached to it.\nRemove unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a simple command for network-related cleanup.\nConnect container to network docker network connect NETWORK CONTAINER Attaches a running container to an existing network. A container can be attached to multiple networks simultaneously.\nConnect with IP docker network connect --ip IP NETWORK CONTAINER Connects a container to a network and assigns it a specific static IP address. This is useful for applications that require predictable IP addresses.\nConnect with alias docker network connect --alias ALIAS NETWORK CONTAINER Assigns a network-scoped alias to a container on the specified network. Other containers on that network can then resolve the container by this ALIAS name.\nDisconnect container docker network disconnect NETWORK CONTAINER Removes a container from a specified network. This can be done on a running container without stopping it.\nRun on specific network docker run --network NAME IMAGE Runs a new container and connects it to a specified network immediately upon creation. If this is omitted, the container defaults to the bridge network.\nRun with network alias docker run --network-alias ALIAS IMAGE Runs a container and sets a network alias that other containers on the same network can use for service discovery. This is a quick way to establish name resolution for the new container.\nRegistry \u0026amp; Authentication Login to registry docker login Logs in to the default public registry (Docker Hub). It prompts you for your username and password.\nLogin to custom registry docker login REGISTRY Logs in to a private or custom registry specified by the REGISTRY URL. This is necessary for pulling and pushing images to non-Docker Hub locations.\nLogin with credentials docker login -u USER -p PASS Logs in using the provided username (-u) and password (-p). Using -p is discouraged for security reasons in interactive shells.\nLogout from registry docker logout Logs out from the default or specified registry. This clears the local credentials stored for the registry.\nPull from custom registry docker pull REGISTRY/IMAGE Pulls an image from a non-Docker Hub registry by including the REGISTRY prefix in the image name. The local host must be logged in to this registry first.\nPush to custom registry docker push REGISTRY/IMAGE Pushes an image to a non-Docker Hub registry. The image must be correctly tagged, and the local host must be logged in.\nTag for registry docker tag IMAGE REGISTRY/IMAGE:TAG Creates a new tag for a local image that includes the full registry path and a TAG. This is the required step before you can push the image to a private registry.\nDocker Compose Start services docker-compose up Reads the docker-compose.yml file, builds or pulls images, and creates and starts all defined services. It runs containers in the foreground and displays the aggregated logs.\nStart in background docker-compose up -d Starts all services in the background (detached mode). This is the most common way to run a multi-container application in production or a long-running environment.\nStop services docker-compose down Stops running containers and removes the containers, networks, and default volumes created by up. This is the standard command for cleaning up a Compose deployment.\nStop and remove volumes docker-compose down -v Performs the standard down cleanup but also explicitly removes the named volumes defined in the Compose file. This permanently deletes persistent data, so use with caution.\nBuild services docker-compose build Builds or re-builds the images for all services that specify a build context in the Compose file. This only executes the build step and does not start the containers.\nBuild without cache docker-compose build --no-cache Forces the build process to ignore any existing build cache for the services. This ensures a fresh, clean build, often used for troubleshooting.\nList services docker-compose ps Displays a list of all containers created by the Compose project. It provides status, command, and port information for all running and stopped services.\nView logs docker-compose logs Displays the aggregated log output for all services in the project. This is the best way to monitor the startup and runtime output of your application.\nFollow logs docker-compose logs -f Streams the log output from all services in real-time. This is similar to docker logs -f but aggregates across all services in the file.\nExecute command docker-compose exec SERVICE COMMAND Runs a command inside a container for a running service (e.g., running a database migration). This command is analogous to docker exec.\nRun one-off command docker-compose run SERVICE COMMAND Runs a one-off command in a new container for a service definition. This is ideal for administrative tasks or tasks that should run and exit (e.g., Django\u0026rsquo;s manage.py runserver).\nStart services docker-compose start Starts existing stopped containers that were created by docker-compose up. This is useful for resuming a project without re-creating all resources.\nStop services docker-compose stop Stops running containers but does not remove them or their networks/volumes. The containers remain on the system and can be restarted later.\nRestart services docker-compose restart Stops and then restarts existing containers for the specified services. This is a convenient way to apply configuration changes that only require a restart.\nPause services docker-compose pause Suspends all processes in the running service containers. The containers remain in memory but their processes are frozen.\nUnpause services docker-compose unpause Resumes the processes in paused service containers. This reverses the effect of the pause command.\nRemove stopped containers docker-compose rm Removes all stopped containers for the project. It is an alternative to down if you only want to clear containers and keep networks/volumes.\nPull service images docker-compose pull Pulls the latest versions of the images referenced in the Compose file from the registry. This updates your local images without starting the containers.\nPush service images docker-compose push Pushes the local images for all services to the configured registry. Requires the images to be tagged with the registry prefix.\nValidate compose file docker-compose config Validates the Compose file syntax and displays the final, normalized configuration. This is useful for debugging and ensuring your file is correctly interpreted.\nScale services docker-compose up --scale SERVICE=NUM Scales a service up or down to the specified number of running instances. This is a quick way to horizontally scale an application on a single host.\nShow running processes docker-compose top Displays the running processes within the service containers. This is an aggregated version of the docker top command for the entire project.\nSystem Management Show Docker info docker info Displays detailed information about the Docker installation and environment. It includes details on storage driver, kernel version, number of images/containers, and available memory.\nShow Docker version docker version Shows the version information for the Docker Client and the Docker Engine (Daemon). This is essential for checking compatibility and troubleshooting version issues.\nShow disk usage docker system df Displays the disk space usage consumed by Docker images, containers, local volumes, and build cache. It gives a clear overview of how much disk space Docker is utilizing.\nRemove unused data docker system prune Removes unused containers, networks, dangling images, and build cache. This is the main command for general Docker system cleanup and reclaiming disk space.\nRemove all unused data docker system prune -a Removes all of the above, plus all non-dangling images that are not referenced by any running container. This performs a much more aggressive cleanup.\nRemove with volumes docker system prune --volumes Adds unused local volumes to the list of items to be removed during the prune. This can free up a significant amount of space but permanently removes persistent data.\nShow real-time events docker events Streams real-time events from the Docker daemon to your terminal. This is used for monitoring container, image, volume, and network activity as it happens.\nFilter events docker events --filter event=TYPE Filters the streamed events to only display events of a specific TYPE (e.g., container, image, die). This helps in focusing the monitoring on relevant activities.\nEvents since time docker events --since TIME Shows all events that have occurred since a specified TIME. This is useful for reviewing a history of events that occurred during a specific period.\nSwarm Management Initialize swarm docker swarm init Initializes a new Docker Swarm on the current machine, making it a Swarm manager. This starts the orchestration capabilities of Docker.\nInitialize with advertise address docker swarm init --advertise-addr IP Initializes the Swarm and specifies the IP address the manager node advertises to other nodes. This is crucial in multi-homed or cloud environments for proper cluster communication.\nJoin as worker docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as a worker node. Worker nodes receive and execute tasks but do not participate in cluster management decisions.\nJoin as manager docker swarm join --token TOKEN IP:PORT Connects the current machine to an existing Swarm as an additional manager node. Multiple managers provide high availability for the control plane.\nLeave swarm docker swarm leave Removes the current machine from the Swarm cluster. If run on a manager, it loses its manager privileges.\nForce leave swarm docker swarm leave -f Forces a manager node to leave the Swarm, even if it is the only remaining manager. This is used when a node needs to be decommissioned without consensus.\nShow join token docker swarm join-token worker Displays the join command and token that a new machine needs to run to join the Swarm as a worker. This token is used for secure authentication.\nShow manager token docker swarm join-token manager Displays the join command and token that a new machine needs to run to join the Swarm as a manager. This token should be kept secure.\nUpdate swarm docker swarm update Updates the configuration of the Swarm cluster. This can be used to set parameters like the task history limit or the auto-lock setting.\nUnlock swarm docker swarm unlock Unlocks the Swarm cluster after it has been explicitly locked by docker swarm update --autolock. This requires the unlock key and is part of a security measure.\nShow unlock key docker swarm unlock-key Displays the unlock key that is required to manually unlock a locked Swarm. This key should be securely backed up immediately after enabling auto-lock.\nStack Management Deploy stack docker stack deploy -c FILE STACK Deploys a multi-service application (a Stack) to the Swarm using a Compose file. This command converts the Compose file into Swarm Services.\nList stacks docker stack ls Lists all of the currently deployed Stacks on the Swarm. It shows the stack name and the number of services running in each stack.\nList stack services docker stack services STACK Lists the individual Swarm services that make up a specified stack. It provides details like the service ID, name, and desired replicas.\nList stack tasks docker stack ps STACK Lists the tasks (running containers) associated with a stack\u0026rsquo;s services. This is used to monitor the state and location of the running application containers.\nRemove stack docker stack rm STACK Removes an entire stack from the Swarm. This gracefully stops and removes all the services and tasks defined in the stack.\nService Management Create service docker service create IMAGE Creates a new Swarm service from an image. A service is the definition of a desired state for one or more tasks (containers).\nList services docker service ls Displays a list of all currently running Swarm services. It shows the service name, mode (replicated/global), and replica status.\nShow service details docker service inspect SERVICE Returns low-level, detailed JSON configuration information about a service. This includes the service\u0026rsquo;s update policy, replicas, and network configuration.\nShow service logs docker service logs SERVICE Aggregates and streams the logs from all the tasks (containers) belonging to a service. This is the primary way to monitor a deployed service in a Swarm.\nList service tasks docker service ps SERVICE Lists the individual tasks (containers) that belong to a service. It shows where each task is running and its current state.\nScale service docker service scale SERVICE=NUM Changes the number of running replicas for a service. Docker Swarm will automatically scale the service up or down to match the new desired NUM.\nUpdate service docker service update SERVICE Updates the configuration of a service (e.g., resource limits, network settings). Docker Swarm will roll out the changes gracefully to the running tasks.\nUpdate image docker service update --image IMAGE SERVICE Updates a service to use a new image version. Swarm initiates a rolling update, replacing old tasks with new ones without downtime.\nRemove service docker service rm SERVICE Removes a running service from the Swarm. This stops and removes all associated tasks and their containers.\nRollback service docker service rollback SERVICE Reverts a service to its previous configuration. This is used when a new update is causing issues and you need a quick way to restore stability.\nNode Management List nodes docker node ls Lists all the nodes (machines) participating in the Swarm cluster. It displays the node ID, hostname, status, and role (manager/worker).\nShow node details docker node inspect NODE Returns detailed JSON information about a specific node. This includes the node\u0026rsquo;s labels, status, availability, and resource specifications.\nUpdate node docker node update NODE Updates the configuration of a node, often used to add or update labels. Labels are used by the Swarm scheduler to constrain service placement.\nPromote to manager docker node promote NODE Changes a worker node\u0026rsquo;s role to a manager node. This is used to add more managers for high availability.\nDemote to worker docker node demote NODE Changes a manager node\u0026rsquo;s role back to a worker node. This is used when you need to reduce the number of managers in the cluster.\nRemove node docker node rm NODE Removes a node from the Swarm cluster. The node must first be drained or explicitly forced to leave.\nList node tasks docker node ps NODE Lists all the tasks (containers) that are currently running on a specific node. This is useful for debugging and checking the workload distribution.\nSet node availability docker node update --availability active|pause|drain NODE Sets the scheduling status of a node, allowing or preventing the Swarm from deploying new tasks to it. drain is used to gracefully move all existing tasks off a node.\nSecret Management Create secret from file docker secret create NAME FILE Creates a new Swarm Secret from the contents of a local FILE. Secrets are securely distributed to only the services that need them.\nCreate secret from stdin echo \u0026#34;data\u0026#34; | docker secret create NAME - Creates a new Swarm Secret from data piped in via standard input (-). This is useful for creating secrets non-interactively in scripts.\nList secrets docker secret ls Lists all the secrets stored securely in the Swarm cluster. It provides the secret ID, name, and creation time.\nShow secret details docker secret inspect NAME Returns detailed JSON information about a secret (metadata only, not the secret content). This is a utility command for inspecting configuration.\nRemove secret docker secret rm NAME Removes a secret from the Swarm cluster. Any service currently using the secret will lose access to it, potentially causing errors.\nConfig Management Create config from file docker config create NAME FILE Creates a new Swarm Config from the contents of a local FILE. Configs are securely distributed configuration files that are read-only for services.\nCreate config from stdin echo \u0026#34;data\u0026#34; | docker config create NAME - Creates a new Swarm Config from data piped in via standard input (-). This is useful for creating small config fragments or for use in scripts.\nList configs docker config ls Lists all the configs stored in the Swarm cluster. It provides the config ID, name, and creation time.\nShow config details docker config inspect NAME Returns detailed JSON information about a config (metadata only, not the config content). This is used to check the config\u0026rsquo;s ID, date, and size.\nRemove config docker config rm NAME Removes a config from the Swarm cluster. Services that were using the config will stop and fail to restart correctly if they still require it.\nPlugin Management (Continued) Upgrade plugin docker plugin upgrade PLUGIN Upgrades an installed plugin to a newer version. This automatically handles the necessary stop/start cycle.\nPush plugin docker plugin push PLUGIN Pushes a locally created plugin to a registry. This is used by plugin developers to distribute their extensions.\nCreate plugin docker plugin create PLUGIN PATH Creates a new plugin from a root filesystem and configuration located in PATH. This is a developer-focused command for packaging a new plugin.\nConfigure plugin docker plugin set PLUGIN KEY=VALUE Sets configuration values for a specific plugin. This is used to fine-tune the plugin\u0026rsquo;s behavior after installation.\nContext Management List contexts docker context ls Lists all the stored Docker contexts. A context defines the endpoint (local daemon, remote server, or cloud provider) that Docker commands run against.\nCreate context docker context create NAME Creates a new Docker context, typically pointing to a remote daemon. This allows you to easily switch between different Docker hosts.\nUse context docker context use NAME Switches the current shell\u0026rsquo;s Docker environment to use the specified context. All subsequent docker commands will execute against the target defined by NAME.\nShow context details docker context inspect NAME Returns detailed JSON information about a specific context. This shows the endpoint and connection details for the target.\nUpdate context docker context update NAME Modifies the connection details or metadata for an existing context. This allows changing the target endpoint without creating a new context.\nRemove context docker context rm NAME Removes a specified context. This only removes the local configuration; it does not affect the remote daemon.\nExport context docker context export NAME Saves a context\u0026rsquo;s configuration to a file. This allows for easy sharing or backup of a context configuration.\nImport context docker context import NAME FILE Imports a context configuration from a specified file. This allows the use of contexts that have been shared or backed up.\nBuilder Management List builders docker builder ls Lists all the available build instances. These instances manage build concurrency and cache.\nPrune build cache docker builder prune Removes the cached data used by the builder instances. This is useful for freeing up disk space consumed by intermediate build layers.\nShow builder disk usage docker builder du Displays a breakdown of the disk space consumed by the build cache. This helps in identifying large or unused cache segments.\nTrust \u0026amp; Content Trust Sign and push image docker trust sign IMAGE:TAG Uses a private key to digitally sign an image and then pushes it to the registry. This confirms the image\u0026rsquo;s origin and integrity.\nShow trust data docker trust inspect IMAGE Displays the signers and trust information for an image in the registry. This confirms whether an image has been signed and by whom.\nRevoke trust docker trust revoke IMAGE:TAG Removes the digital signature for a specified image from the registry. This is used to invalidate a signed image.\nGenerate trust key docker trust key generate NAME Generates a new cryptographic key pair (private and public) for signing images. The private key is used for signing, and the public key is used for verification.\nLoad trust key docker trust key load FILE Adds a private key from a file to the local user\u0026rsquo;s trust repository. This allows the user to sign images using the imported key.\nAdd signer docker trust signer add --key FILE SIGNER REPO Adds a signer (user/key) to a repository, allowing that key to sign images for that repository. The FILE points to the public key of the new signer.\nRemove signer docker trust signer remove SIGNER REPO Removes a signer\u0026rsquo;s public key from a repository. The removed signer can no longer sign images for that repository.\nCheckpoint Management Create checkpoint docker checkpoint create CONTAINER NAME Creates a checkpoint (a snapshot of the running container\u0026rsquo;s state) using CRIU. This allows the container to be restored to that exact state later.\nList checkpoints docker checkpoint ls CONTAINER Lists all the checkpoints created for a specific container. This shows the available restoration points.\nRemove checkpoint docker checkpoint rm CONTAINER NAME Deletes a specific checkpoint for a container. This removes the snapshot data from the host filesystem.\nManifest Management Inspect manifest docker manifest inspect IMAGE Displays the manifest list or manifest for a multi-architecture image. This shows which architectures and operating systems the image supports.\nCreate manifest docker manifest create LIST IMAGE... Creates a manifest list that references multiple architecture-specific images. This is used to create a single, multi-platform image name.\nPush manifest docker manifest push LIST Pushes the newly created manifest list to the registry. This makes the multi-architecture image available for platforms to automatically pull the correct image.\nAnnotate manifest docker manifest annotate LIST IMAGE --os OS --arch ARCH Adds or updates metadata (like OS or architecture) for an image within a manifest list. This ensures proper platform selection when pulling the multi-arch image.\nContainer Cleanup Remove all stopped containers docker container prune Removes all exited or stopped containers. This is a quick way to clean up resources from terminated container runs.\nRemove containers older than docker container prune --filter \u0026#34;until=24h\u0026#34; Removes stopped containers that have been inactive for longer than the specified time (e.g., 24 hours). This allows for a targeted cleanup of old resources.\nRemove with label filter docker container prune --filter \u0026#34;label=KEY\u0026#34; Removes stopped containers based on whether they have a specific label assigned to them. This enables filtering by custom metadata.\nImage Cleanup Remove dangling images docker image prune Removes only the dangling images (those that are not tagged and not referenced by any container). This is the safest cleanup of unneeded image layers.\nRemove all unused images docker image prune -a Removes all unused images, including intermediate and non-dangling images not referenced by a container. This is a powerful command that frees up significant disk space.\nRemove images older than docker image prune -a --filter \u0026#34;until=24h\u0026#34; Removes all unused images that were created or last modified longer than the specified time (e.g., 24 hours) ago. This provides an age-based policy for image cleanup.\nVolume Cleanup Remove all unused volumes docker volume prune Removes all local volumes that are not currently referenced by any container. This is the standard, single command for safely removing orphaned volume data.\nRemove with filter docker volume prune --filter \u0026#34;label=KEY\u0026#34; Removes unused volumes that have a specific label assigned to them. This allows for targeted removal of volumes based on custom metadata.\nNetwork Cleanup Remove all unused networks docker network prune Removes all local, user-defined networks that are not currently in use by any container. This is a quick way to clean up orphaned network configurations.\nRemove with filter docker network prune --filter \u0026#34;until=24h\u0026#34; Removes unused networks that were created longer than the specified time (e.g., 24 hours) ago. This helps maintain a clean network environment based on age.\nCommon Run Options Run with port mapping docker run -p HOST:CONTAINER IMAGE Maps a port from the container (CONTAINER) to a port on the host machine (HOST). This allows external access to the service running inside the container.\nRun with environment variable docker run -e KEY=VALUE IMAGE Sets an environment variable inside the container. This is a primary method for configuring applications at runtime.\nRun with multiple env vars docker run -e KEY1=VAL1 -e KEY2=VAL2 IMAGE Sets multiple environment variables for the container\u0026rsquo;s environment. The -e flag must be repeated for each variable.\nRun with env file docker run --env-file FILE IMAGE Sets environment variables using key-value pairs listed in a FILE. This is a cleaner way to pass a large number of variables.\nRun with working directory docker run -w PATH IMAGE Sets the working directory inside the container where the primary command will be executed. This is useful for commands that expect to run from a specific location.\nRun as specific user docker run -u USER IMAGE Runs the container\u0026rsquo;s primary process with a specified USER or UID/GID. This is an important security feature to avoid running as the root user.\nRun with hostname docker run -h HOSTNAME IMAGE Sets the hostname for the container. This value is often used by the application for identification or logging purposes.\nRun with CPU limit docker run --cpus NUM IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s CPU resources to a specific number of cores. This is a crucial command for resource governance and preventing a container from hogging the CPU.\nRun with memory limit docker run -m SIZE IMAGE Limits the container\u0026rsquo;s access to the host\u0026rsquo;s memory to a specific SIZE (e.g., 512m, 1g). This prevents a memory leak in one container from crashing the entire host.\nRun with privileged mode docker run --privileged IMAGE Gives the container full access to the host\u0026rsquo;s devices and kernel capabilities. This should be used with extreme caution as it breaks container isolation.\nRun with cap add docker run --cap-add CAPABILITY IMAGE Adds a specific Linux CAPABILITY to the container\u0026rsquo;s set of privileges. This grants fine-grained access to kernel features without using the full --privileged flag.\nRun with cap drop docker run --cap-drop CAPABILITY IMAGE Removes a specific Linux CAPABILITY from the container\u0026rsquo;s set of privileges. This is used to further reduce the security attack surface.\nRun with device docker run --device PATH IMAGE Adds a host device (e.g., a GPU, or a serial port) to the container. This is necessary for containers that need to interact directly with hardware.\nRun with restart policy docker run --restart=always IMAGE Sets the restart policy for the container (e.g., always, on-failure). always ensures the container restarts automatically whenever it exits.\nRun with label docker run -l KEY=VALUE IMAGE Adds metadata (a label) to the container. Labels are often used for organization, monitoring, or automating cleanup scripts.\nRun with link docker run --link CONTAINER IMAGE DEPRECATED: Links a container to another via the legacy network mechanism. Modern practice uses user-defined networks for DNS resolution instead.\nRun with DNS docker run --dns IP IMAGE Configures a custom DNS server IP for the container to use for name resolution. This overrides the host\u0026rsquo;s default DNS settings.\nRun with add-host docker run --add-host HOST:IP IMAGE Adds an entry to the container\u0026rsquo;s /etc/hosts file. This is useful for custom host-to-IP mappings inside the container\u0026rsquo;s environment.\nRun with pid mode docker run --pid host IMAGE Allows the container to share the PID namespace with the host system. This lets the container see all processes running on the host, which is a significant security risk but necessary for some monitoring tools.\nRun with IPC mode docker run --ipc host IMAGE Allows the container to share the IPC namespace with the host system. This enables inter-process communication between the container and processes on the host.\nAdditional Tips Tip: Use docker COMMAND --help for detailed options and examples for any Docker command.\nAnd after hours of focus, trial, and error, I finally completed the sheet. I couldn’t help but shed a few tears — a mix of relief, pride, and sheer satisfaction.\n","permalink":"http://localhost:1313/posts/d5ad17a7c7a95be35679fb94b2ab25e3/","summary":"\u003ch1 id=\"docker-cli-cheat-sheet-complete-reference-guide\"\u003eDocker CLI Cheat Sheet: Complete Reference Guide\u003c/h1\u003e\n\u003ch2 id=\"container-lifecycle\"\u003eContainer Lifecycle\u003c/h2\u003e\n\u003ch3 id=\"run-a-container-from-an-image\"\u003eRun a container from an image\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run \u003cspan class=\"o\"\u003e[\u003c/span\u003eOPTIONS\u003cspan class=\"o\"\u003e]\u003c/span\u003e IMAGE \u003cspan class=\"o\"\u003e[\u003c/span\u003eCOMMAND\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreates and starts a container from a specified IMAGE. This is the primary command for getting a container up and running, allowing you to execute an optional COMMAND inside it.\u003c/p\u003e\n\u003ch3 id=\"run-container-in-background\"\u003eRun container in background\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -d IMAGE\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRuns the container in detached mode, meaning it runs in the background. The command prints the container ID and exits the terminal, allowing you to continue using your shell.\u003c/p\u003e","title":"Docker CLI Cheat Sheet: More then you need to be PRO."},{"content":"Introduction The inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\nWhen you create a filesystem in Linux, a specific portion of the disk is reserved for the inode table. This table is essentially a fixed-size array of inodes, with each inode containing detailed metadata about a file. The number of inodes is determined when the filesystem is created, based on factors like the filesystem size and expected usage patterns. This means you can theoretically run out of inodes even if you have plenty of disk space left, which happens when you create too many small files.\nPractical Example:\n# Check inode usage on your system $ df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda1 6553600 450000 6103600 7% / /dev/sdb1 524288 45000 479288 9% /home # View inode numbers for files $ ls -i /etc/passwd 1234567 /etc/passwd What an Inode Contains Each inode is roughly 256 bytes in size and contains a wealth of information about a file. It stores the file type, whether it\u0026rsquo;s a regular file, directory, symbolic link, or device file. The permission bits are stored here, including the read, write, and execute permissions for the owner, group, and others, along with special bits like setuid, setgid, and the sticky bit. Ownership information is tracked through user ID and group ID fields.\nTimestamps are a critical part of the inode. Every file has an access time that records when it was last read, a modification time that shows when the content changed, and a change time that updates whenever the inode metadata itself changes. Modern filesystems like ext4 also support a birth time that records when the file was originally created. The inode also maintains a link count, which tracks how many directory entries point to this particular inode. When this count drops to zero, the filesystem knows it can safely reclaim the space.\nThe most complex part of an inode is how it stores references to the actual data blocks on disk. Traditional ext filesystems use a clever hierarchical scheme with twelve direct pointers that point straight to data blocks, perfect for small files. For larger files, there\u0026rsquo;s an indirect pointer that points to a block full of pointers, effectively extending the file\u0026rsquo;s capacity. Even larger files use double indirect and triple indirect pointers, creating a tree structure that can address massive amounts of data while keeping small files efficient.\nPractical Example:\n# Use stat to see all inode information $ stat /etc/passwd File: /etc/passwd Size: 2847 Blocks: 8 IO Block: 4096 regular file Device: 803h/2051d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2025-10-04 10:23:15.123456789 +0530 Modify: 2025-10-04 10:23:15.123456789 +0530 Change: 2025-10-04 10:23:15.123456789 +0530 Birth: 2025-10-01 08:15:30.987654321 +0530 # View block allocation pattern for a file $ filefrag -v /var/log/syslog Filesystem type is: ef53 File size of /var/log/syslog is 1048576 (256 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 127: 1234568.. 1234695: 128: 1: 128.. 255: 2345678.. 2345805: 128: 1234696: last,eof # Find files by their inode number $ find /etc -inum 1234567 /etc/passwd How Inodes Work in Practice Every file and directory in a Linux filesystem has a unique inode number within that filesystem. You can see these numbers using the ls -i command. The root directory always gets inode number 2, and the numbering continues from there. When you create a new file, the filesystem finds a free inode from its bitmap, fills it with metadata, and creates a directory entry that maps the filename to the inode number.\nHard links are an interesting consequence of the inode system. When you create a hard link, you\u0026rsquo;re simply creating another directory entry that points to the same inode. Both filenames share the exact same data and metadata because they\u0026rsquo;re pointing to the same inode. This is why modifying one hard-linked file immediately affects all others with the same inode number. The link count in the inode keeps track of how many names point to it, and only when this reaches zero can the file truly be deleted.\nSymbolic links work differently. A symbolic link is actually a special type of file that has its own inode. Instead of pointing to data blocks with file content, its data blocks contain the path to another file. This is why symbolic links can break if the target file is moved or deleted, while hard links cannot break as long as one link remains.\nPractical Example:\n# Create a test file and examine its inode $ echo \u0026#34;Original content\u0026#34; \u0026gt; original.txt $ ls -li original.txt 9876543 -rw-r--r-- 1 user user 17 Oct 04 11:05 original.txt # Note the inode number (9876543) and link count (1) # Create a hard link - same inode, increased link count $ ln original.txt hardlink.txt $ ls -li original.txt hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 hardlink.txt 9876543 -rw-r--r-- 2 user user 17 Oct 04 11:05 original.txt # Same inode number! Link count is now 2 # Modify through hard link - both files change $ echo \u0026#34;Modified\u0026#34; \u0026gt;\u0026gt; hardlink.txt $ cat original.txt Original content Modified # Create a symbolic link - different inode $ ln -s original.txt symlink.txt $ ls -li symlink.txt 1111111 lrwxrwxrwx 1 user user 12 Oct 04 11:07 symlink.txt -\u0026gt; original.txt # Different inode number (1111111) # Delete original - hard link still works, symlink breaks $ rm original.txt $ cat hardlink.txt Original content Modified $ cat symlink.txt cat: symlink.txt: No such file or directory Filesystem Operations and Inodes Understanding inodes helps explain why certain file operations behave the way they do. When you move a file within the same filesystem, Linux simply updates the directory entry to point to the existing inode in a new location. The inode number doesn\u0026rsquo;t change, and no data is copied, making it an extremely fast operation. However, moving a file across different filesystems requires creating a new inode on the destination, copying all the data, and then deleting the original, which is much slower.\nThe stat command reveals all the information stored in an inode. When you run it on a file, you see the inode number, file size, number of blocks allocated, permissions, ownership, and all the timestamps. This command literally reads the inode and presents its contents in a human-readable format.\nPractical Example:\n# Create a test file $ echo \u0026#34;Test data\u0026#34; \u0026gt; testfile.txt $ stat testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Move within same filesystem - inode doesn\u0026#39;t change $ mv testfile.txt /tmp/testfile.txt $ stat /tmp/testfile.txt | grep Inode Device: 803h/2051d Inode: 5555555 Links: 1 # Same inode number! Only directory entry changed # Time the operation - it\u0026#39;s instant $ time mv /tmp/testfile.txt /tmp/renamed.txt real 0m0.001s # Moving across filesystems creates new inode $ mv /tmp/renamed.txt /mnt/other_disk/ $ stat /mnt/other_disk/renamed.txt | grep Inode Device: 804h/2052d Inode: 7777777 Links: 1 # Different device and inode number - data was copied Common Issues and Considerations One of the most frustrating problems system administrators encounter is inode exhaustion. This occurs when you\u0026rsquo;ve used up all available inodes even though disk space remains. It typically happens on systems that accumulate many small files, such as mail servers or systems with poor log rotation. The error message says \u0026ldquo;No space left on device\u0026rdquo; which is technically true, but it\u0026rsquo;s the inode space that\u0026rsquo;s exhausted, not disk space. You can check inode usage with df -i to see both total inodes and how many are in use.\nDifferent filesystems handle inodes differently. Traditional ext filesystems allocate all inodes at creation time, which is why you need to choose the right inode ratio for your use case. XFS takes a more modern approach with dynamic inode allocation, creating them on demand as files are created. This eliminates the possibility of running out of inodes while having free disk space. Btrfs and ZFS use even more advanced structures that integrate checksums and other features directly into their inode-equivalent structures.\nPractical Example:\n# Check for inode exhaustion $ df -h /dev/sdb1 Filesystem Size Used Avail Use% Mounted on /dev/sdb1 100G 20G 80G 20% /mnt/data $ df -i /dev/sdb1 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sdb1 524288 524288 0 100% /mnt/data # 100% inodes used but only 20% disk space! # Find directories with most files $ sudo find /mnt/data -xdev -type d -exec sh -c \\ \u0026#39;echo $(ls -A1 \u0026#34;$1\u0026#34; | wc -l) \u0026#34;$1\u0026#34;\u0026#39; _ {} \\; | sort -rn | head -5 45678 /mnt/data/mail/user1/.Trash 34521 /mnt/data/cache/sessions 23456 /mnt/data/tmp/uploads 12345 /mnt/data/logs/old 10234 /mnt/data/mail/user2/cur # Create filesystem with more inodes $ sudo mkfs.ext4 -i 4096 /dev/sdb1 # One inode per 4KB $ sudo mkfs.ext4 -N 2000000 /dev/sdb1 # Exactly 2 million inodes # Check inode settings on existing filesystem $ sudo tune2fs -l /dev/sda1 | grep -i inode Inode count: 6553600 Free inodes: 6103600 Inodes per group: 8192 Inode size: 256 Performance and Caching Linux maintains an inode cache in RAM to speed up file operations. When you access a file, its inode is loaded into memory and kept there for quick access. This is why the second time you run ls -l in a directory, it\u0026rsquo;s noticeably faster than the first time. The inode cache is part of the Virtual File System layer, which provides a unified interface across different filesystem types.\nThe size of inodes can be configured when creating a filesystem, and larger inodes offer advantages like storing extended attributes inline and supporting nanosecond precision timestamps. However, they also consume more space. The default 256-byte inode size in ext4 is a reasonable compromise for most workloads.\nPractical Example:\n# Check inode cache statistics $ cat /proc/sys/fs/inode-nr 105234 8945 # Total allocated inodes | Free inodes in cache $ cat /proc/sys/fs/inode-state 105234 8945 0 0 0 0 0 # Clear caches to see performance difference $ sudo sync \u0026amp;\u0026amp; sudo sysctl -w vm.drop_caches=2 $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.234s # Run again with warm cache $ time ls -l /usr/bin \u0026gt; /dev/null real 0m0.012s # Much faster with cached inodes! # Use debugfs to examine inode details (requires root) $ sudo debugfs -R \u0026#34;stat \u0026lt;1234567\u0026gt;\u0026#34; /dev/sda1 Inode: 1234567 Type: regular Mode: 0644 Flags: 0x80000 Generation: 3456789012 Version: 0x00000000:00000001 User: 0 Group: 0 Size: 2847 File ACL: 0 Directory ACL: 0 Links: 1 Blockcount: 8 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 atime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 mtime: 0x6525ab3f:1e8b4d88 -- Fri Oct 4 10:23:15 2025 crtime: 0x6522f1d2:3c4e5a7b -- Tue Oct 1 08:15:30 2025 Size of extra inode fields: 32 EXTENTS: (0):1234568 Advanced Techniques Finding All Hard Links to a File:\n# Get inode number $ inode_num=$(stat -c %i myfile.txt) $ echo $inode_num 9876543 # Find all files with same inode $ find /home -xdev -inum $inode_num 2\u0026gt;/dev/null /home/user/myfile.txt /home/user/documents/backup.txt /home/user/archive/old_version.txt Recovering Deleted Files:\n# If a process still has the file open $ lsof | grep deleted apache 1234 www-data 3w REG 8,1 1048576 9999999 /tmp/logfile (deleted) # Recover via /proc filesystem $ cp /proc/1234/fd/3 /tmp/recovered_logfile Working with Extended Attributes:\n# Set custom metadata $ setfattr -n user.comment -v \u0026#34;Important document\u0026#34; file.txt $ setfattr -n user.project -v \u0026#34;Project-X\u0026#34; file.txt # View extended attributes $ getfattr -d file.txt # file: file.txt user.comment=\u0026#34;Important document\u0026#34; user.project=\u0026#34;Project-X\u0026#34; # Copy file preserving extended attributes $ cp --preserve=xattr file.txt backup.txt Conclusion The inode table represents decades of refined filesystem design, providing a robust foundation for file management in Linux. By separating file metadata from filenames and data, the inode system enables features like hard links, efficient file operations, and flexible storage management. Whether you\u0026rsquo;re troubleshooting a mysterious \u0026ldquo;disk full\u0026rdquo; error that isn\u0026rsquo;t really about disk space, or optimizing a filesystem for millions of small files, understanding inodes gives you insight into how Linux truly manages your data at the lowest level.\nThis knowledge transforms you from someone who uses a filesystem to someone who understands it, making you a more effective system administrator or developer. The practical commands and examples provided throughout this guide give you the tools to explore, monitor, and troubleshoot inode-related issues in real-world scenarios.\n","permalink":"http://localhost:1313/posts/4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe inode table is one of the most fundamental data structures in Linux filesystems, yet it remains mysterious to many users. An inode, short for index node, is essentially a data structure that stores all the metadata about a file or directory except for two crucial pieces of information: the filename itself and the actual file content. This separation of concerns is what makes Unix-like filesystems elegant and efficient.\u003c/p\u003e","title":"Understanding Linux Filesystem Inode Tables"},{"content":"Introduction Why wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\nModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\nThis is where Docker comes to the rescue. Docker packages software into standardized units called containers, which include everything the application needs to run—code, libraries, system tools, and runtime. By using Docker, applications can be deployed quickly and consistently across any environment, making scaling easier and ensuring that the code runs reliably every time.\nWhat is Containers ? Containers are a technology that allow applications to be packaged and isolated with their entire runtime environment. This makes it easier to maintain consistent behavior and functionality while moving the contained application between environments (dev, test, production) and across public, private, hybrid cloud, and on-premise. Because they are lightweight and portable, containers provide opportunities for faster development and meeting business needs as they arise.\nBare-metal/VM Provides full isolation and a complete OS, but is heavier, slower to start, and harder to scaling where as container is lightweight, fast to start, and easily scalable, making it ideal for DevOps pipelines and micro-services.\nNote:\nVirtual Machines (VMs) provide strong isolation and security because each VM runs a full OS, making it harder for one VM to affect another. Containers, on the other hand, share the host OS kernel, so while they are lightweight and fast, their isolation is weaker, and extra security measures are often needed.\nWhat is Docker ? Docker is an open-source platform that enables developers to build, deploy, run, update and manage containers. Docker is a way to build and run those containers, save them into templates etc.\nDocker = the tool/platform to build, ship, and manage containers. Docker Image = a read-only blueprint that contains everything needed to create a container, including code, libraries, and dependencies. Container = a running instance of a Docker image; an isolated, portable environment where your application actually runs. Open Container Initiative (OCI) The OCI is an industry collaboration that aims to create open standards for container formats. It was founded by companies like Docker, Google, VMware, Microsoft, Dell, IBM, and Oracle. The OCI defines three primary specifications:\nImage Specification: Defines the image\u0026rsquo;s metadata and format, including a serializable file system. Runtime Specification: Describes how to run a container using an image adhering to the Image Specification. Distribution Specification: Outlines how images should be distributed, such as through registries, pushing, and pulling images. Building Blocks Linux namespaces Namespaces are a Linux kernel feature that partitions system resources so that one group of processes sees one set of resources, while another group sees a different set. This is achieved by assigning processes and resources to namespaces—each namespace provides an isolated view, even though they exist on the same system.\nIn general terms, a namespace is a container for commands and variables. It ensures that the commands and variables inside it do not conflict with those in other namespaces.\nFor example, in Tcl, there has always been a default namespace called the global namespace. It stores all global variables and commands. New namespaces can be created using the namespace eval command, allowing for better organization and isolation of variables and commands. Read more\nCommands Some namespace-related commands lsns # List all namespaces on the system. ls -l /proc/1/ns # Check namespaces attached to a specific process (e.g., PID 1). unshare -p -f --mount-proc bash # Start a new shell with a new PID namespace. unshare -p -u -m -f --mount-proc bash # Combine multiple namespaces (PID, UTS, mount). ip netns add testns # Create network Namespaces unshare -U -r bash #Map a non-root user ro root inside the namespaces. # The namespace eval command lets you create new namespaces. namespace eval Counter { namespace export bump variable num 0 proc bump {} { variable num incr num } } namespace eval Counter { variable num 0 proc bump {} { variable num return [incr num] } } namespace eval Counter { proc test {args} { return $args } } namespace eval Counter { rename test \u0026#34;\u0026#34; } Note that the test procedure is added to the Counter namespace, and later removed via the rename command.\ncgroups unionFilesystem ( overlay-filesystem) Let the Chaos Begin Installation of Docker Docker can be installed on a variety of operating systems, and the installation process differs slightly depending on your platform. On Arch Linux, Docker is available directly from the official repositories. You can install it using the following commands:\nsudo pacman -Syu docker sudo systemctl enable --now docker On Ubuntu or other Debian-based distributions, Docker provides an official installation script or repository-based installation method, which ensures you get the latest stable version of Docker. For Windows and macOS, Docker provides Docker Desktop, a complete package that includes the Docker engine, Docker CLI, and Docker Compose. Installing Docker Desktop is as simple as downloading the installer from the official Docker website and following the setup instructions. On Windows, Docker Desktop can use WSL2 to run Linux containers, while macOS uses a lightweight Linux VM under the hood.\nDockerFile Running Your fist Docker Container docker run docker/whalesay cowsay \u0026ldquo;Hey Team! 👋\u0026rdquo;\n","permalink":"http://localhost:1313/posts/e3e0502f6219a9c5edb5e39c4e70f043/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eWhy wrestle with messy setups when you can box your app, ship it anywhere, and sip tea while it just works? No more “it works on my machine” excuses. Let’s breeze through Docker—no deep dives, just the good stuff.\u003c/p\u003e\n\u003cp\u003eModern applications are no longer simple. They rely on specific versions of libraries, frameworks, and configurations to run correctly. Setting up an application on one machine and then trying to run it on another often leads to the classic “it works on my machine” problem, frustrating both developers and operations teams.\u003c/p\u003e","title":"Docker in a Nutshell: Box It, Ship It, Forget It"},{"content":"One of the most powerful and elegant features of the Linux command line is the pipe, represented by the vertical bar: |.\nIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\nWhat Does the Pipe (|) Do? In the simplest terms, the pipe takes the output of one command and feeds it directly as the input to a second command.\nThink of it like an assembly line:\n$$\\text{Command 1} \\quad \\rightarrow \\quad \\textbf{Output} \\quad \\rightarrow \\quad \\text{Pipe } (|) \\quad \\rightarrow \\quad \\textbf{Input} \\quad \\rightarrow \\quad \\text{Command 2}$$\nThe beauty of this is that instead of having to save the results of the first command to a temporary file, the data flows instantly from one utility to the next.\n3 Practical Examples Here are a few ways you can use the pipe to make your work on the command line faster and more efficient.\n1. Counting Files of a Specific Type Let\u0026rsquo;s say you want to quickly count all the .txt files in a directory.\nCommand 1 (ls -l): Lists all files and details. Command 2 (grep \u0026quot;.txt\u0026quot;): Filters the list to only show lines containing .txt. Command 3 (wc -l): Counts the number of lines (which is the count of your files). ls -l | grep \u0026#34;.txt\u0026#34; | wc -l Instead of manually counting, you get the exact number instantly!\n2. Finding a Running Program If you know a program is running, but you don\u0026rsquo;t want to sift through the entire process list, use the pipe to filter the results.\nCommand 1 (ps aux): Lists all running processes on the system. Command 2 (grep \u0026quot;firefox\u0026quot;): Filters that list to only show lines mentioning the string \u0026ldquo;firefox.\u0026rdquo; ps aux | grep \u0026#34;firefox\u0026#34; This is much quicker than scrolling through hundreds of lines of processes. (A quick tip: you might need to use grep -v grep to exclude the grep command itself from the output, but the basic command above is a great starting point!)\n3. Reading Large Files Easily The cat command displays the entire contents of a file all at once, which is fine for small files, but terrible for large ones. You can pipe the output to a pager like less.\nCommand 1 (cat /var/log/syslog): Tries to print the entire system log. Command 2 (less): Allows you to scroll and search through the output page-by-page. cat /var/log/syslog | less Now you can read that massive log file comfortably without it flying past your screen.\nConclusion The pipe (|) is an essential concept for anyone who uses the Linux command line. It embodies the core Unix philosophy: \u0026ldquo;Write programs that do one thing and do it well.\u0026rdquo;\nBy chaining these single-purpose commands together, you gain limitless flexibility and power, making complex tasks simple. Now go ahead and try piping a few commands together!\n","permalink":"http://localhost:1313/posts/a2cc786af0d345abcacce0e9c5097a6a/","summary":"\u003cp\u003eOne of the most powerful and elegant features of the Linux command line is the \u003cstrong\u003epipe\u003c/strong\u003e, represented by the vertical bar: \u003ccode\u003e|\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt looks simple, but this little symbol is the secret weapon for turning a handful of small, simple commands into a single, powerful tool.\u003c/p\u003e\n\u003ch2 id=\"what-does-the-pipe--do\"\u003eWhat Does the Pipe (\u003ccode\u003e|\u003c/code\u003e) Do?\u003c/h2\u003e\n\u003cp\u003eIn the simplest terms, the pipe takes the \u003cstrong\u003eoutput of one command and feeds it directly as the input to a second command.\u003c/strong\u003e\u003c/p\u003e","title":"The Power of the Pipe: Chaining Linux Commands for Efficiency"},{"content":"Listen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, no shade. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my Arch Linux era, and the vibes are just\u0026hellip; chef\u0026rsquo;s kiss.\nThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro slays at, and why I made the switch from a stable icon to the ultimate DIY challenge.\nUbuntu: The Friend Who Has It All Together For real, Ubuntu is the LTS King. It\u0026rsquo;s the distribution you recommend to anyone migrating from Windows or macOS. Why?\nIt just works: The installation is GUI-based, smooth, and genuinely low-effort. You click a few buttons, and boom—you have a fully functioning desktop environment, office suite, and media apps. It’s a complete package. Stability is the assignment: Ubuntu\u0026rsquo;s Long-Term Support (LTS) releases are certified reliable. If you\u0026rsquo;re running a server, a school lab, or just need your laptop to never pull a fast one on you, Ubuntu is the safe bet. Widespread Support: The community is huge. If you hit a bug, a quick Google search will give you a result from the Ubuntu forums that\u0026rsquo;s years old but still fixes your problem. Verdict: Ubuntu is the user-friendly, reliable powerhouse. It walks so other distros can run. It’s the OG for a reason. I could never hate it. It taught me how to use the terminal without the fear of bricking my whole setup.\nArch Linux: The \u0026ldquo;Built Different\u0026rdquo; Philosophy Switching to Arch was less about needing better performance and more about a vibe shift. It aligns with a deep-seated desire for control and minimalism that just hits with our generation.\n1. The \u0026ldquo;KISS\u0026rdquo; Principle is a whole mood Arch follows the Keep It Simple, Stupid (KISS) principle—but not simple to use, simple in design. When you install Arch, you get a minimal base system. Nothing extra. No bloat. No pre-installed Snap packages you didn\u0026rsquo;t ask for. It’s a clean slate. You install only what you need, which means your system is leaner, faster, and you know exactly what is running. This intentional minimalism is underrated.\n2. The Rolling Release is the ultimate flex Ubuntu has fixed releases (every six months, with LTS every two years). That means you have to wait for the latest kernel, the newest desktop environment, or the freshest package version. Arch? It’s rolling release. The moment a package is deemed stable by the Arch maintainers, it drops.\nTranslation: You are always running the latest and greatest. No more waiting six months for a new feature in your code editor or a performance bump in your graphics driver. It’s bleeding edge—and while that means you sometimes gotta troubleshoot, that’s part of the fun. 3. Pacman and the AUR are the main characters apt (Ubuntu’s package manager) is fine, but Pacman is lightning fast. And the AUR (Arch User Repository)? It\u0026rsquo;s the real game-changer. It’s a community-driven repository with build scripts for basically every single piece of software you could ever want. If it exists for Linux, it’s probably in the AUR. This eliminates the headache of adding sketchy PPAs (Personal Package Archives) like you often have to on Ubuntu to get niche or cutting-edge apps.\n4. You Understand Your System (Final Boss Level) The famously difficult command-line installation of Arch forces you to learn how Linux works from the ground up: partitioning drives, mounting filesystems, setting up the bootloader, and configuring the kernel. After you finish, you don\u0026rsquo;t just use Linux, you know Linux. It\u0026rsquo;s a rite of passage, and for anyone serious about development or system administration, that foundational knowledge is priceless.\nThe Final Takeaway (No Cap) Ubuntu is the stable, reliable car that gets you to work every day without fail. Arch is the high-performance, custom-built race car that you maintain yourself. I use Arch because I value the control, the minimal base, and the instant access to the latest software via the AUR. It lets me build a system that is perfectly tailored to my workflow.\nBut seriously, if you\u0026rsquo;re looking for an easy start, Ubuntu is still peak performance for beginners. There\u0026rsquo;s a distro for every stage of your journey.\nIf you want an in-depth visual breakdown comparing the two major philosophies, check out this video: Ubuntu vs Arch Linux: Ultimate Linux Distro Comparison for Beginners \u0026amp; Pros!. This video provides a great side-by-side comparison of Ubuntu and Arch Linux, touching on the key differences in installation and target audience. http://googleusercontent.com/youtube_content/0\n","permalink":"http://localhost:1313/posts/ffd8bd1512001a08fb1f9279a6caaab6/","summary":"\u003cp\u003eListen up, fam. We need to talk about my operating system journey. I started on Ubuntu, and honestly, \u003cstrong\u003eno shade\u003c/strong\u003e. It\u0026rsquo;s the ultimate soft launch into the Linux world. But now? I\u0026rsquo;m firmly in my \u003cstrong\u003eArch Linux\u003c/strong\u003e era, and the vibes are just\u0026hellip; \u003cem\u003echef\u0026rsquo;s kiss\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a \u0026ldquo;flex culture\u0026rdquo; post. It\u0026rsquo;s about recognizing what each distro \u003cem\u003eslays\u003c/em\u003e at, and why I made the switch from a stable icon to the ultimate DIY challenge.\u003c/p\u003e","title":"Why Arch Is My Main Character Energy (But Ubuntu is Still the GOAT Starter Pack)"}]